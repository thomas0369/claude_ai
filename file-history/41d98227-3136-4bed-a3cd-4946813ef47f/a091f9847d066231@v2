#!/usr/bin/env python3
"""
ZEREZ Import Base Classes
=========================

Shared functionality for all ZEREZ import scripts.
Provides centralized stats tracking, file I/O, progress streaming, and error handling.
"""

import json
import logging
import os
import time
from pathlib import Path
from typing import Dict, List, Optional
from abc import ABC, abstractmethod

logger = logging.getLogger(__name__)


class ZEREZImportStats:
    """Centralized stats tracking for all ZEREZ import operations."""

    def __init__(self, stream_progress: bool = False):
        self.stream_progress = stream_progress
        self.stats = {
            'total': 0,
            'downloaded': 0,
            'failed': 0,
            'saved_files': 0,
            'current': '',
            'errors': []
        }

    def emit(self):
        """Emit progress update as JSON (for SSE streaming)."""
        if self.stream_progress:
            print(json.dumps(self.stats), flush=True)

    def update(self, **kwargs):
        """Update stats and emit if streaming."""
        self.stats.update(kwargs)
        self.emit()

    def increment(self, field: str, amount: int = 1):
        """Increment a numeric stat field."""
        self.stats[field] = self.stats.get(field, 0) + amount
        self.emit()

    def add_error(self, error: str):
        """Add an error message."""
        self.stats['errors'].append(error)
        self.emit()


class ZEREZFileWriter:
    """Handles file writing and verification for ZEREZ data."""

    def __init__(self, output_dir: Path):
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def save_json(
        self,
        unit_id: str,
        data: Dict,
        model_name: str = None
    ) -> tuple[bool, Optional[str]]:
        """
        Save unit data to JSON file with verification.

        Args:
            unit_id: UUID of the unit
            data: Complete unit data to save
            model_name: Optional display name for logging

        Returns:
            (success: bool, error_message: Optional[str])
        """
        output_file = None
        display_name = model_name or unit_id

        try:
            output_file = self.output_dir / f"{unit_id}.json"

            # Write with explicit flush for WSL2/network filesystem compatibility
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
                f.flush()  # Force flush to OS buffer
                os.fsync(f.fileno())  # Force OS to commit to disk

            # Verify file was saved (with retry for slow filesystems)
            # WSL2 and network drives may have delayed directory metadata updates
            max_retries = 3
            for attempt in range(max_retries):
                if output_file.exists() and output_file.stat().st_size > 0:
                    file_size_kb = output_file.stat().st_size / 1024
                    logger.info(f"âœ“ Saved {display_name} ({file_size_kb:.1f} KB)")
                    return True, None
                elif attempt < max_retries - 1:
                    # Wait briefly for filesystem to catch up (WSL2/network drive issue)
                    time.sleep(0.01)  # 10ms delay

            # All retries failed
            return False, (
                f"File verification failed for {display_name}: file not found after write. "
                f"This may indicate a filesystem issue (WSL2, network drive, or disk full). "
                f"Check: {output_file}"
            )

        except PermissionError as e:
            return False, (
                f"Permission denied saving {display_name} to {output_file}. "
                f"Check directory permissions for: {self.output_dir}"
            )

        except OSError as e:
            return False, (
                f"Disk error saving {display_name} to {output_file}: {str(e)}. "
                f"Check available disk space or file system limits."
            )

        except json.JSONDecodeError as e:
            return False, (
                f"Invalid JSON data for {display_name}: {str(e)}. "
                f"Field causing issue: {getattr(e, 'msg', 'unknown')} "
                f"at position {getattr(e, 'pos', 'unknown')}"
            )

        except Exception as e:
            return False, (
                f"Failed to save {display_name}: {type(e).__name__}: {str(e)}. "
                f"Unit ID: {unit_id}, Output: {output_file}"
            )

    def save_bulk_json(
        self,
        units: List[Dict],
        metadata: Optional[Dict] = None
    ) -> tuple[bool, Optional[str], Optional[str]]:
        """
        Save all units to a single timestamped JSON file.

        Args:
            units: List of complete unit data objects
            metadata: Optional metadata to include (import time, filters, etc.)

        Returns:
            (success: bool, error_message: Optional[str], filename: Optional[str])
        """
        try:
            # Generate timestamped filename
            timestamp = time.strftime('%Y-%m-%d_%H-%M-%S')
            filename = f"zerez_import_{timestamp}.json"
            output_file = self.output_dir / filename

            # Build output structure
            output_data = {
                "metadata": metadata or {},
                "products": units
            }

            # Ensure metadata has basic info
            if "importedAt" not in output_data["metadata"]:
                output_data["metadata"]["importedAt"] = time.strftime('%Y-%m-%d %H:%M:%S')
            if "totalProducts" not in output_data["metadata"]:
                output_data["metadata"]["totalProducts"] = len(units)
            if "importMethod" not in output_data["metadata"]:
                output_data["metadata"]["importMethod"] = "graphql_api_search"

            # Write with explicit flush for WSL2/network filesystem compatibility
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(output_data, f, indent=2, ensure_ascii=False)
                f.flush()  # Force flush to OS buffer
                os.fsync(f.fileno())  # Force OS to commit to disk

            # Verify file was saved (with retry for slow filesystems)
            max_retries = 3
            for attempt in range(max_retries):
                if output_file.exists() and output_file.stat().st_size > 0:
                    file_size_kb = output_file.stat().st_size / 1024
                    file_size_mb = file_size_kb / 1024
                    if file_size_mb >= 1:
                        logger.info(f"âœ“ Saved bulk file: {filename} ({file_size_mb:.2f} MB, {len(units)} products)")
                    else:
                        logger.info(f"âœ“ Saved bulk file: {filename} ({file_size_kb:.1f} KB, {len(units)} products)")
                    return True, None, filename
                elif attempt < max_retries - 1:
                    time.sleep(0.01)  # 10ms delay for filesystem

            # All retries failed
            return False, (
                f"File verification failed for bulk export: file not found after write. "
                f"This may indicate a filesystem issue (WSL2, network drive, or disk full). "
                f"Check: {output_file}"
            ), None

        except PermissionError:
            return False, (
                f"Permission denied saving bulk export to {self.output_dir}. "
                f"Check directory permissions."
            ), None

        except OSError as e:
            return False, (
                f"Disk error saving bulk export: {str(e)}. "
                f"Check available disk space or file system limits."
            ), None

        except Exception as e:
            return False, (
                f"Failed to save bulk export: {type(e).__name__}: {str(e)}"
            ), None


class ZEREZImporterBase(ABC):
    """Base class for all ZEREZ importers."""

    def __init__(self, output_dir: Path, stream_progress: bool = False):
        self.output_dir = output_dir
        self.stats = ZEREZImportStats(stream_progress)
        self.writer = ZEREZFileWriter(output_dir)

    @abstractmethod
    def process_unit(self, unit_data: Dict, model_name: str) -> bool:
        """
        Process a single unit. Must be implemented by subclasses.

        Args:
            unit_data: Complete unit data from GraphQL or API
            model_name: Display name for logging

        Returns:
            True if successful, False otherwise
        """
        pass

    def process_units(self, units: List[Dict]) -> Dict:
        """Process multiple units with progress tracking."""
        self.stats.update(total=len(units))
        logger.info(f"ðŸš€ Processing {len(units)} units...")
        logger.info(f"   Output directory: {self.output_dir}")

        start_time = time.time()

        for i, unit_data in enumerate(units, 1):
            unit_id = unit_data.get('id', f'Unit {i}')
            model_name = unit_data.get('modelName', unit_id)

            logger.info(f"[{i}/{len(units)}] Processing {model_name}...")
            self.stats.update(current=model_name)

            if self.process_unit(unit_data, model_name):
                self.stats.increment('downloaded')
            else:
                self.stats.increment('failed')

        elapsed_time = time.time() - start_time

        # Final summary
        self.stats.update(current='Complete')
        logger.info(f"\n{'='*60}")
        logger.info(f"âœ… Processing Complete!")
        logger.info(f"   Total: {self.stats.stats['total']}")
        logger.info(f"   Success: {self.stats.stats['downloaded']}")
        logger.info(f"   Failed: {self.stats.stats['failed']}")
        logger.info(f"   Saved files: {self.stats.stats['saved_files']}")
        logger.info(f"   Time: {elapsed_time:.1f}s ({elapsed_time/len(units):.2f}s per unit)")
        logger.info(f"{'='*60}")

        if self.stats.stats['errors']:
            logger.warning(f"\nâš ï¸  Errors encountered:")
            for error in self.stats.stats['errors'][:5]:
                logger.warning(f"   - {error}")
            if len(self.stats.stats['errors']) > 5:
                logger.warning(f"   ... and {len(self.stats.stats['errors']) - 5} more")

        return {
            'total': self.stats.stats['total'],
            'downloaded': self.stats.stats['downloaded'],
            'failed': self.stats.stats['failed'],
            'saved_files': self.stats.stats['saved_files'],
            'errors': self.stats.stats['errors'],
            'elapsed_seconds': elapsed_time,
            'output_directory': str(self.output_dir)
        }


def parse_unit_input(json_string: str) -> tuple[List[Dict], Dict[str, str]]:
    """
    Parse unit data from JSON string (handles both ID arrays and object arrays).

    Args:
        json_string: JSON string containing unit data

    Returns:
        (unit_list, id_to_name_map)

    Raises:
        ValueError: If input format is invalid
    """
    try:
        data = json.loads(json_string)

        if not isinstance(data, list):
            raise ValueError("Input must be a JSON array")

        if not data:
            raise ValueError("Input array is empty")

        # Validate structure
        for i, item in enumerate(data):
            if not isinstance(item, dict):
                raise ValueError(f"Item {i} is not a JSON object")
            if 'id' not in item:
                raise ValueError(f"Item {i} missing 'id' field")

        # Build ID to name mapping
        id_map = {item['id']: item.get('modelName', item['id']) for item in data}

        return data, id_map

    except json.JSONDecodeError as e:
        raise ValueError(f"Invalid JSON: {e}")


def validate_output_directory(output_dir: Path) -> None:
    """
    Validate that output directory can be created.

    Args:
        output_dir: Path to output directory

    Raises:
        ValueError: If parent directory doesn't exist
    """
    if not output_dir.parent.exists():
        raise ValueError(f"Parent directory does not exist: {output_dir.parent}")
