#!/usr/bin/env python3
"""
ZEREZ JSON Saver
================

Saves ZEREZ unit data to JSON files.
Designed to work with data already fetched from GraphQL search.

This script receives complete unit data from the search results
and saves each unit to an individual JSON file with all 58 fields.

Usage:
    python3 zerez_json_saver.py --units '[...]' --output-dir data/zerez/imports --stream-progress
"""

import argparse
import json
import logging
import sys
import time
from pathlib import Path
from typing import List, Dict

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class ZEREZJSONSaver:
    """Save ZEREZ product data to JSON files."""

    def __init__(self, output_dir: Path, stream_progress: bool = False):
        """Initialize saver."""
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.stream_progress = stream_progress

        self.stats = {
            'total': 0,
            'saved': 0,
            'failed': 0,
            'current': '',
            'errors': []
        }

    def emit_progress(self):
        """Emit progress update as JSON (for SSE streaming)."""
        if self.stream_progress:
            print(json.dumps(self.stats), flush=True)

    def save_unit(self, unit_data: Dict) -> bool:
        """
        Save a single unit to JSON file.

        Args:
            unit_data: Complete unit data from GraphQL (all 58 fields)

        Returns:
            True if saved successfully, False otherwise
        """
        try:
            unit_id = unit_data.get('id')
            model_name = unit_data.get('modelName', unit_id)

            if not unit_id:
                raise ValueError("Unit data missing 'id' field")

            self.stats['current'] = model_name
            self.emit_progress()

            # Add metadata
            enhanced_data = {
                **unit_data,
                'importedAt': time.strftime('%Y-%m-%d %H:%M:%S'),
                'importMethod': 'graphql_api_search',
                'apiVersion': '1.0',
            }

            # Save to JSON file
            output_file = self.output_dir / f"{unit_id}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(enhanced_data, f, indent=2, ensure_ascii=False)

            # Verify file was saved
            if output_file.exists() and output_file.stat().st_size > 0:
                file_size_kb = output_file.stat().st_size / 1024
                logger.info(f"‚úì Saved {model_name} ({file_size_kb:.1f} KB)")
                self.stats['saved'] += 1
                self.emit_progress()
                return True
            else:
                raise Exception("File verification failed: file not saved or empty")

        except Exception as e:
            error_msg = f"Failed to save {model_name}: {str(e)}"
            logger.error(f"‚úó {error_msg}")

            self.stats['failed'] += 1
            self.stats['errors'].append(error_msg)
            self.emit_progress()

            return False

    def save_units(self, units: List[Dict]) -> Dict:
        """Save multiple units."""
        self.stats['total'] = len(units)
        self.emit_progress()

        logger.info(f"üíæ Saving {len(units)} products to JSON...")
        logger.info(f"   Output directory: {self.output_dir}")

        start_time = time.time()

        for i, unit_data in enumerate(units, 1):
            model_name = unit_data.get('modelName', unit_data.get('id', f'Unit {i}'))
            logger.info(f"[{i}/{len(units)}] Saving {model_name}...")
            self.save_unit(unit_data)

        elapsed_time = time.time() - start_time

        # Emit final progress
        self.stats['current'] = 'Complete'
        self.emit_progress()

        # Log summary
        logger.info(f"\n" + "="*60)
        logger.info(f"‚úÖ Save Complete!")
        logger.info(f"   Total: {self.stats['total']}")
        logger.info(f"   Saved: {self.stats['saved']}")
        logger.info(f"   Failed: {self.stats['failed']}")
        logger.info(f"   Time: {elapsed_time:.1f}s ({elapsed_time/len(units):.2f}s per product)")
        logger.info(f"   Output: {self.output_dir}")
        logger.info(f"="*60)

        if self.stats['errors']:
            logger.warning(f"\n‚ö†Ô∏è  Errors encountered:")
            for error in self.stats['errors'][:5]:
                logger.warning(f"   - {error}")
            if len(self.stats['errors']) > 5:
                logger.warning(f"   ... and {len(self.stats['errors']) - 5} more")

        return {
            'total': self.stats['total'],
            'saved': self.stats['saved'],
            'failed': self.stats['failed'],
            'errors': self.stats['errors'],
            'elapsed_seconds': elapsed_time,
            'output_directory': str(self.output_dir)
        }


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description='Save ZEREZ product data to JSON files',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Save products from search results
  python3 zerez_json_saver.py \\
    --units '[{"id":"uuid1","modelName":"Model 1",...}]' \\
    --output-dir data/zerez/imports \\
    --stream-progress

Input Format:
  --units expects a JSON array of complete unit objects from GraphQL search.
  Each unit should contain all 58 fields returned by the API.

Output Format:
  Each product is saved as {uuid}.json with:
  - All original fields from GraphQL
  - importedAt timestamp
  - importMethod metadata
        """
    )
    parser.add_argument('--units', required=True, help='JSON array of complete unit data')
    parser.add_argument('--output-dir', required=True, help='Output directory for JSON files')
    parser.add_argument('--stream-progress', action='store_true', help='Stream progress updates (for SSE)')

    args = parser.parse_args()

    # Parse unit data
    try:
        units = json.loads(args.units)

        if not isinstance(units, list):
            raise ValueError("units must be a JSON array")

        if not units:
            raise ValueError("units array is empty")

        # Validate each unit has required fields
        for i, unit in enumerate(units):
            if not isinstance(unit, dict):
                raise ValueError(f"Unit {i} is not a JSON object")
            if 'id' not in unit:
                raise ValueError(f"Unit {i} missing 'id' field")

    except json.JSONDecodeError as e:
        print(json.dumps({'error': f'Invalid JSON: {e}'}), file=sys.stderr)
        sys.exit(1)
    except ValueError as e:
        print(json.dumps({'error': str(e)}), file=sys.stderr)
        sys.exit(1)

    # Validate output directory parent exists
    output_dir = Path(args.output_dir)
    if not output_dir.parent.exists():
        print(json.dumps({'error': f'Parent directory does not exist: {output_dir.parent}'}), file=sys.stderr)
        sys.exit(1)

    # Run saver
    saver = ZEREZJSONSaver(output_dir, stream_progress=args.stream_progress)

    try:
        result = saver.save_units(units)

        # Print final result (if not streaming)
        if not args.stream_progress:
            print(json.dumps(result, indent=2))

        # Exit with error code if any failures
        if result['failed'] > 0:
            sys.exit(1)

    except KeyboardInterrupt:
        logger.warning("\n‚ö†Ô∏è  Save interrupted by user")
        error_data = {'error': 'Interrupted by user', 'stats': saver.stats}
        print(json.dumps(error_data), file=sys.stderr)
        sys.exit(130)

    except Exception as e:
        logger.error(f"‚ùå Fatal error: {e}")
        error_data = {'error': str(e), 'stats': saver.stats}
        print(json.dumps(error_data), file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()
