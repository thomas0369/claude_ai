#!/usr/bin/env python3
"""
ZEREZ Detail Importer
=====================

Downloads and parses HTML detail pages for ZEREZ products.
Designed to stream progress updates for real-time UI feedback.

Usage:
    python3 zerez_detail_importer.py --product-ids '[...]' --output-dir data/zerez/imports --stream-progress
"""

import argparse
import json
import logging
import sys
import time
from pathlib import Path
from typing import List, Dict, Optional
import asyncio
from playwright.async_api import async_playwright, Page
from bs4 import BeautifulSoup

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class ZEREZDetailImporter:
    """Import ZEREZ product details from HTML pages."""

    def __init__(self, output_dir: Path, stream_progress: bool = False):
        """Initialize importer."""
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.stream_progress = stream_progress

        self.base_url = "https://www.zerez.net"
        self.stats = {
            'total': 0,
            'downloaded': 0,
            'failed': 0,
            'saved_files': 0,  # Track actual files saved to disk
            'current': '',
            'errors': []
        }

    def emit_progress(self):
        """Emit progress update as JSON (for SSE streaming)."""
        if self.stream_progress:
            print(json.dumps(self.stats), flush=True)

    async def download_detail_page(self, page: Page, product_id: str, model_name: str) -> Optional[Dict]:
        """Download and parse a single detail page."""
        url = f"{self.base_url}/units/{product_id}"

        try:
            self.stats['current'] = model_name
            self.emit_progress()

            # Navigate to detail page
            await page.goto(url, wait_until='networkidle', timeout=30000)

            # Wait for content to load
            await page.wait_for_selector('.MuiCircularProgress-root', state='hidden', timeout=10000)
            await asyncio.sleep(2)  # Buffer for dynamic content

            # Get page HTML
            html = await page.content()

            # Parse with BeautifulSoup
            soup = BeautifulSoup(html, 'html.parser')

            # Extract product details (adjust selectors based on actual ZEREZ HTML)
            details = {
                'id': product_id,
                'modelName': model_name,
                'url': url,
                'html': html,
                'extractedData': self._extract_specs(soup),
                'downloadedAt': time.strftime('%Y-%m-%d %H:%M:%S')
            }

            # Save to JSON
            output_file = self.output_dir / f"{product_id}.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(details, f, indent=2, ensure_ascii=False)

            # Verify file was actually saved
            if output_file.exists() and output_file.stat().st_size > 0:
                self.stats['saved_files'] += 1
            else:
                error_msg = f"File verification failed for {model_name}: file not saved or empty"
                logger.error(f"⚠ {error_msg}")
                self.stats['errors'].append(error_msg)

            self.stats['downloaded'] += 1
            self.emit_progress()

            logger.info(f"✓ Downloaded: {model_name}")
            return details

        except Exception as e:
            error_msg = f"Failed to download {model_name}: {str(e)}"
            logger.error(f"✗ {error_msg}")

            self.stats['failed'] += 1
            self.stats['errors'].append(error_msg)
            self.emit_progress()

            return None

    def _extract_specs(self, soup: BeautifulSoup) -> Dict:
        """Extract specifications from HTML."""
        specs = {}

        # Find all specification tables
        # This is a simplified example - adjust based on actual ZEREZ HTML structure
        tables = soup.find_all('table')

        for table in tables:
            rows = table.find_all('tr')
            for row in rows:
                cells = row.find_all(['th', 'td'])
                if len(cells) == 2:
                    key = cells[0].get_text(strip=True)
                    value = cells[1].get_text(strip=True)
                    specs[key] = value

        # Extract from data attributes
        elements_with_data = soup.find_all(attrs={'data-field': True})
        for elem in elements_with_data:
            field = elem.get('data-field')
            value = elem.get_text(strip=True) or elem.get('data-value')
            if field and value:
                specs[field] = value

        return specs

    async def import_products(self, product_ids: List[str], product_models: Dict[str, str]):
        """Import multiple products."""
        self.stats['total'] = len(product_ids)
        self.emit_progress()

        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context(
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            )
            page = await context.new_page()

            for product_id in product_ids:
                model_name = product_models.get(product_id, product_id)
                await self.download_detail_page(page, product_id, model_name)

            await browser.close()

        # Emit final progress
        self.stats['current'] = 'Complete'
        self.emit_progress()

        return {
            'total': self.stats['total'],
            'downloaded': self.stats['downloaded'],
            'failed': self.stats['failed'],
            'saved_files': self.stats['saved_files'],
            'errors': self.stats['errors']
        }


def main():
    """Main entry point."""
    parser = argparse.ArgumentParser(description='Import ZEREZ product details')
    parser.add_argument('--product-ids', required=True, help='JSON array of product IDs')
    parser.add_argument('--output-dir', required=True, help='Output directory')
    parser.add_argument('--stream-progress', action='store_true', help='Stream progress updates')

    args = parser.parse_args()

    # Parse product IDs
    try:
        product_data = json.loads(args.product_ids)

        # Handle different input formats
        if isinstance(product_data, list):
            # List of strings (just IDs)
            if all(isinstance(p, str) for p in product_data):
                product_ids = product_data
                product_models = {pid: pid for pid in product_ids}
            # List of objects (ID + model name)
            else:
                product_ids = [p['id'] for p in product_data]
                product_models = {p['id']: p.get('modelName', p['id']) for p in product_data}
        else:
            raise ValueError("Invalid product data format")

    except json.JSONDecodeError as e:
        print(json.dumps({'error': f'Invalid JSON: {e}'}), file=sys.stderr)
        sys.exit(1)

    # Run importer
    output_dir = Path(args.output_dir)
    importer = ZEREZDetailImporter(output_dir, stream_progress=args.stream_progress)

    try:
        result = asyncio.run(importer.import_products(product_ids, product_models))

        # Print final result
        if not args.stream_progress:
            print(json.dumps(result, indent=2))

    except Exception as e:
        error_data = {'error': str(e), 'stats': importer.stats}
        print(json.dumps(error_data), file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()
