"""Data Cache Manager - Minimize API traffic with local data storage"""

import os
import sys
import pandas as pd
import json
from datetime import datetime, timedelta
from typing import Optional, List, Dict
import logging
from pathlib import Path

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DataCacheManager:
    """
    Manages local data cache to minimize API traffic

    Strategy:
    - Store 104 days of 30min data per asset (~4992 bars, maximum for TradingView)
    - Resample to higher timeframes (1h, 45min, etc.) without API calls
    - Incrementally update: fetch only new bars since last update
    - Persist metadata to track last update timestamps
    """

    CACHE_DIR = "data/market_data"
    METADATA_FILE = "data/market_data/cache_metadata.json"
    BASE_TIMEFRAME = "30min"
    MAX_HISTORY_DAYS = 104  # 104 days (~4992 bars) - maximum for 30min within TradingView's 5000 bar limit

    def __init__(self):
        """Initialize cache manager"""
        self.cache_dir = Path(self.CACHE_DIR)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        self.metadata_file = Path(self.METADATA_FILE)
        self.metadata = self._load_metadata()

    def _load_metadata(self) -> Dict:
        """Load cache metadata"""
        if self.metadata_file.exists():
            with open(self.metadata_file, 'r') as f:
                return json.load(f)
        return {}

    def _save_metadata(self):
        """Save cache metadata"""
        with open(self.metadata_file, 'w') as f:
            json.dump(self.metadata, f, indent=2)

    def _get_cache_path(self, symbol: str, timeframe: str = None) -> Path:
        """Get cache file path for symbol"""
        tf = timeframe or self.BASE_TIMEFRAME
        safe_symbol = symbol.replace('/', '_')
        return self.cache_dir / f"{safe_symbol}_{tf}.csv"

    def get_last_update(self, symbol: str) -> Optional[datetime]:
        """Get last update timestamp for symbol"""
        if symbol in self.metadata:
            return datetime.fromisoformat(self.metadata[symbol]['last_update'])
        return None

    def _set_last_update(self, symbol: str, timestamp: datetime):
        """Set last update timestamp for symbol"""
        if symbol not in self.metadata:
            self.metadata[symbol] = {}
        self.metadata[symbol]['last_update'] = timestamp.isoformat()
        self.metadata[symbol]['bars_count'] = self.get_bars_count(symbol)
        self._save_metadata()

    def get_bars_count(self, symbol: str, timeframe: str = None) -> int:
        """Get number of cached bars for symbol"""
        cache_path = self._get_cache_path(symbol, timeframe)
        if cache_path.exists():
            df = pd.read_csv(cache_path)
            return len(df)
        return 0

    def load_from_cache(
        self,
        symbol: str,
        timeframe: str = "1h",
        bars: int = 200
    ) -> Optional[pd.DataFrame]:
        """
        Load data from cache

        Args:
            symbol: Asset symbol
            timeframe: Desired timeframe (will resample from base if needed)
            bars: Number of bars to return

        Returns:
            DataFrame with OHLCV data or None
        """
        try:
            # Check if we have base timeframe data
            base_cache_path = self._get_cache_path(symbol, self.BASE_TIMEFRAME)

            if not base_cache_path.exists():
                logger.warning(f"No cache for {symbol} at {self.BASE_TIMEFRAME}")
                return None

            # Load base data
            df = pd.read_csv(base_cache_path, parse_dates=['timestamp'])
            df.set_index('timestamp', inplace=True)

            if len(df) == 0:
                return None

            # Resample if needed
            if timeframe != self.BASE_TIMEFRAME:
                df = self._resample_timeframe(df, timeframe)

            # Return requested number of bars
            return df.tail(bars)

        except Exception as e:
            logger.error(f"Error loading cache for {symbol}: {e}")
            return None

    def _resample_timeframe(self, df: pd.DataFrame, target_timeframe: str) -> pd.DataFrame:
        """
        Resample data to target timeframe

        Supports: 1h, 45min, 4h, 1d
        """
        # Map timeframe strings to pandas offset aliases (updated for pandas 2.0+)
        timeframe_map = {
            '30min': '30min',
            '45min': '45min',
            '1h': '1h',
            '4h': '4h',
            '1d': '1D'
        }

        if target_timeframe not in timeframe_map:
            logger.warning(f"Unsupported timeframe {target_timeframe}, returning original")
            return df

        freq = timeframe_map[target_timeframe]

        # Resample OHLCV data
        resampled = df.resample(freq).agg({
            'open': 'first',
            'high': 'max',
            'low': 'min',
            'close': 'last',
            'volume': 'sum'
        }).dropna()

        return resampled

    def update_cache(
        self,
        symbol: str,
        data_provider,
        force_full_refresh: bool = False
    ) -> bool:
        """
        Update cache with new data from provider

        Args:
            symbol: Asset symbol
            data_provider: Data provider instance (e.g., CapitalDataProvider)
            force_full_refresh: If True, fetch full history (104 days)

        Returns:
            True if successful
        """
        try:
            cache_path = self._get_cache_path(symbol, self.BASE_TIMEFRAME)

            # Determine fetch strategy
            if force_full_refresh or not cache_path.exists():
                # Full fetch: 104 days of 30min data (~4992 bars)
                bars_to_fetch = self.MAX_HISTORY_DAYS * 48  # 48 bars per day for 30min
                logger.info(f"üì• Full refresh for {symbol}: fetching {bars_to_fetch} bars ({self.MAX_HISTORY_DAYS} days)")

                new_data = data_provider.fetch_ohlcv(symbol, self.BASE_TIMEFRAME, bars_to_fetch)

                if new_data is None or len(new_data) == 0:
                    logger.error(f"Failed to fetch data for {symbol}")
                    return False

                # Save to cache
                new_data.reset_index(inplace=True)
                new_data.rename(columns={'index': 'timestamp'}, inplace=True)
                new_data.to_csv(cache_path, index=False)

                last_timestamp = pd.to_datetime(new_data['timestamp'].iloc[-1])
                self._set_last_update(symbol, last_timestamp)

                logger.info(f"‚úÖ Cached {len(new_data)} bars for {symbol}")
                return True

            else:
                # Incremental update: fetch only new bars
                last_update = self.get_last_update(symbol)

                if last_update is None:
                    logger.warning(f"No last update timestamp for {symbol}, doing full refresh")
                    return self.update_cache(symbol, data_provider, force_full_refresh=True)

                # Calculate bars to fetch (since last update + some buffer)
                time_since_update = datetime.now() - last_update
                bars_to_fetch = max(10, int(time_since_update.total_seconds() / 1800) + 5)  # 1800s = 30min

                logger.info(f"üì• Incremental update for {symbol}: fetching {bars_to_fetch} new bars")

                new_data = data_provider.fetch_ohlcv(symbol, self.BASE_TIMEFRAME, bars_to_fetch)

                if new_data is None or len(new_data) == 0:
                    logger.warning(f"No new data for {symbol}")
                    return True  # Not an error, just no new data

                # Load existing cache
                existing_df = pd.read_csv(cache_path, parse_dates=['timestamp'])

                # Merge with new data (avoid duplicates)
                new_data_reset = new_data.reset_index()
                new_data_reset.rename(columns={'index': 'timestamp'}, inplace=True)

                # Combine and drop duplicates
                combined = pd.concat([existing_df, new_data_reset], ignore_index=True)
                combined['timestamp'] = pd.to_datetime(combined['timestamp'])
                combined.drop_duplicates(subset=['timestamp'], keep='last', inplace=True)
                combined.sort_values('timestamp', inplace=True)

                # Keep only last 104 days
                cutoff_date = datetime.now() - timedelta(days=self.MAX_HISTORY_DAYS)
                combined = combined[combined['timestamp'] >= cutoff_date]

                # Save back to cache
                combined.to_csv(cache_path, index=False)

                last_timestamp = pd.to_datetime(combined['timestamp'].iloc[-1])
                self._set_last_update(symbol, last_timestamp)

                new_bars_added = len(combined) - len(existing_df)
                logger.info(f"‚úÖ Added {new_bars_added} new bars to {symbol} cache (total: {len(combined)})")
                return True

        except Exception as e:
            logger.error(f"Error updating cache for {symbol}: {e}", exc_info=True)
            return False

    def update_all_symbols(
        self,
        symbols: List[str],
        data_provider,
        force_full_refresh: bool = False
    ) -> Dict[str, bool]:
        """
        Update cache for all symbols

        Returns:
            Dict mapping symbol to success status
        """
        results = {}

        logger.info(f"üîÑ Updating cache for {len(symbols)} symbols...")

        for symbol in symbols:
            logger.info(f"\n{'='*60}")
            logger.info(f"Processing: {symbol}")
            logger.info(f"{'='*60}")

            success = self.update_cache(symbol, data_provider, force_full_refresh)
            results[symbol] = success

            if success:
                last_update = self.get_last_update(symbol)
                bars_count = self.get_bars_count(symbol)
                logger.info(f"üìä {symbol}: {bars_count} bars | Last: {last_update.strftime('%Y-%m-%d %H:%M') if last_update else 'N/A'}")

        # Summary
        successful = sum(1 for v in results.values() if v)
        logger.info(f"\n{'='*60}")
        logger.info(f"‚úÖ Cache update complete: {successful}/{len(symbols)} successful")
        logger.info(f"{'='*60}\n")

        return results

    def get_cache_stats(self) -> Dict:
        """Get statistics about cached data"""
        stats = {
            'total_symbols': len(self.metadata),
            'symbols': {}
        }

        for symbol, info in self.metadata.items():
            last_update = datetime.fromisoformat(info['last_update'])
            age_hours = (datetime.now() - last_update).total_seconds() / 3600

            stats['symbols'][symbol] = {
                'bars': info.get('bars_count', 0),
                'last_update': last_update.isoformat(),
                'age_hours': round(age_hours, 1),
                'needs_update': age_hours > 1  # Update if older than 1 hour
            }

        return stats

    def clear_cache(self, symbol: Optional[str] = None):
        """Clear cache for symbol or all symbols"""
        if symbol:
            cache_path = self._get_cache_path(symbol, self.BASE_TIMEFRAME)
            if cache_path.exists():
                cache_path.unlink()
                if symbol in self.metadata:
                    del self.metadata[symbol]
                    self._save_metadata()
                logger.info(f"üóëÔ∏è  Cleared cache for {symbol}")
        else:
            # Clear all
            for file in self.cache_dir.glob("*.csv"):
                file.unlink()
            self.metadata = {}
            self._save_metadata()
            logger.info(f"üóëÔ∏è  Cleared all cache")


def main():
    """CLI for cache management"""
    import argparse

    parser = argparse.ArgumentParser(description='Market Data Cache Manager')
    parser.add_argument('--action', choices=['update', 'stats', 'clear'], required=True,
                        help='Action to perform')
    parser.add_argument('--symbol', help='Specific symbol (optional)')
    parser.add_argument('--full-refresh', action='store_true',
                        help='Force full refresh (104 days)')
    parser.add_argument('--demo', action='store_true',
                        help='Use demo data provider')

    args = parser.parse_args()

    manager = DataCacheManager()

    if args.action == 'stats':
        stats = manager.get_cache_stats()
        print(f"\nüìä Cache Statistics")
        print(f"{'='*60}")
        print(f"Total symbols: {stats['total_symbols']}")
        print(f"\nPer Symbol:")
        for symbol, info in stats['symbols'].items():
            status = 'üî¥ NEEDS UPDATE' if info['needs_update'] else 'üü¢ OK'
            print(f"  {symbol:12s} | {info['bars']:5d} bars | Age: {info['age_hours']:5.1f}h | {status}")
        print(f"{'='*60}\n")

    elif args.action == 'clear':
        manager.clear_cache(args.symbol)

    elif args.action == 'update':
        # Get data provider
        if args.demo:
            # Use demo data
            from multi_strategy_screener import MultiStrategyScreener
            screener = MultiStrategyScreener(use_demo_data=True)

            # Mock provider for demo
            class DemoProvider:
                def __init__(self, screener):
                    self.screener = screener

                def fetch_ohlcv(self, symbol, timeframe, bars):
                    return self.screener.fetch_data(symbol, timeframe, bars)

            provider = DemoProvider(screener)
            symbols = [args.symbol] if args.symbol else screener.DEFAULT_ASSETS
        else:
            # Use real provider
            from capital_data_provider import CapitalDataProvider
            provider = CapitalDataProvider()
            if args.symbol:
                symbols = [args.symbol]
            else:
                from config import ALL_ASSETS
                symbols = ALL_ASSETS

        results = manager.update_all_symbols(symbols, provider, args.full_refresh)

        # Print summary
        successful = sum(1 for v in results.values() if v)
        print(f"\n‚úÖ Update complete: {successful}/{len(results)} successful\n")


if __name__ == '__main__':
    main()
