"""
Persistent Data Cache Manager for Elliott Wave Trading Bot.

Provides:
- Parquet-based storage for OHLCV data
- Metadata tracking (last_updated, data ranges, candle counts)
- Gap detection and filling
- Incremental updates with overlap validation
- Thread-safe operations
"""

import json
import logging
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Dict, Optional, Tuple, List
import pandas as pd
import numpy as np
from threading import Lock

logger = logging.getLogger(__name__)


class DataCache:
    """Manages persistent caching of OHLCV data with gap detection."""

    def __init__(self, cache_dir: str = "data/cache"):
        """
        Initialize the data cache.

        Args:
            cache_dir: Root directory for cache storage
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self._locks: Dict[str, Lock] = {}  # Per-symbol locks for thread safety

    def _get_lock(self, symbol: str) -> Lock:
        """Get or create a lock for a symbol."""
        if symbol not in self._locks:
            self._locks[symbol] = Lock()
        return self._locks[symbol]

    def _get_symbol_dir(self, symbol: str) -> Path:
        """Get cache directory for a symbol."""
        # Replace / with _ for file system compatibility
        safe_symbol = symbol.replace('/', '_')
        symbol_dir = self.cache_dir / safe_symbol
        symbol_dir.mkdir(parents=True, exist_ok=True)
        return symbol_dir

    def _get_data_path(self, symbol: str, timeframe: str) -> Path:
        """Get parquet file path for symbol/timeframe."""
        return self._get_symbol_dir(symbol) / f"{timeframe}.parquet"

    def _get_metadata_path(self, symbol: str) -> Path:
        """Get metadata JSON path for symbol."""
        return self._get_symbol_dir(symbol) / "metadata.json"

    def load_metadata(self, symbol: str) -> Dict:
        """
        Load metadata for a symbol.

        Returns:
            Metadata dict with timeframe info, or empty dict if not cached
        """
        metadata_path = self._get_metadata_path(symbol)
        if not metadata_path.exists():
            return {}

        try:
            with open(metadata_path, 'r') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Error loading metadata for {symbol}: {e}")
            return {}

    def save_metadata(self, symbol: str, metadata: Dict) -> None:
        """Save metadata for a symbol."""
        metadata_path = self._get_metadata_path(symbol)
        try:
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)
        except Exception as e:
            logger.error(f"Error saving metadata for {symbol}: {e}")

    def load_data(self, symbol: str, timeframe: str) -> Optional[pd.DataFrame]:
        """
        Load cached data for symbol/timeframe.

        Args:
            symbol: Asset symbol (e.g., "EUR/USD")
            timeframe: Timeframe (e.g., "5m", "15m", "1h")

        Returns:
            DataFrame with OHLCV data, or None if not cached
        """
        data_path = self._get_data_path(symbol, timeframe)
        if not data_path.exists():
            return None

        try:
            df = pd.read_parquet(data_path)
            # Ensure timestamp is datetime index
            if 'timestamp' in df.columns:
                df['timestamp'] = pd.to_datetime(df['timestamp'])
                df.set_index('timestamp', inplace=True)
            return df
        except Exception as e:
            logger.error(f"Error loading data for {symbol} {timeframe}: {e}")
            return None

    def save_data(self, symbol: str, timeframe: str, df: pd.DataFrame) -> None:
        """
        Save data to cache.

        Args:
            symbol: Asset symbol
            timeframe: Timeframe
            df: DataFrame with OHLCV data (timestamp as index)
        """
        with self._get_lock(symbol):
            data_path = self._get_data_path(symbol, timeframe)

            try:
                # Ensure timestamp is in index
                if 'timestamp' not in df.index.names:
                    if 'timestamp' in df.columns:
                        df = df.set_index('timestamp')

                # Save to parquet
                df.to_parquet(data_path, compression='snappy')

                # Update metadata
                metadata = self.load_metadata(symbol)
                if 'timeframes' not in metadata:
                    metadata['timeframes'] = {}

                metadata['timeframes'][timeframe] = {
                    'last_updated': datetime.now(timezone.utc).isoformat(),
                    'candle_count': len(df),
                    'first_candle': df.index.min().isoformat(),
                    'last_candle': df.index.max().isoformat()
                }

                self.save_metadata(symbol, metadata)
                logger.info(f"Saved {len(df)} candles for {symbol} {timeframe}")

            except Exception as e:
                logger.error(f"Error saving data for {symbol} {timeframe}: {e}")

    def merge_data(
        self,
        cached_df: pd.DataFrame,
        new_df: pd.DataFrame,
        overlap_candles: int = 10
    ) -> Tuple[pd.DataFrame, bool]:
        """
        Merge cached and new data with overlap validation.

        Args:
            cached_df: Existing cached data
            new_df: Newly fetched data
            overlap_candles: Number of candles to validate for continuity

        Returns:
            (merged_df, is_valid) tuple
            - merged_df: Combined data without duplicates
            - is_valid: True if overlap validation passed
        """
        if cached_df is None or len(cached_df) == 0:
            return new_df, True

        if new_df is None or len(new_df) == 0:
            return cached_df, True

        # Ensure both have datetime index
        if not isinstance(cached_df.index, pd.DatetimeIndex):
            cached_df.index = pd.to_datetime(cached_df.index)
        if not isinstance(new_df.index, pd.DatetimeIndex):
            new_df.index = pd.to_datetime(new_df.index)

        # Find overlap period
        cached_last = cached_df.index.max()
        new_first = new_df.index.min()

        # Validate overlap if new data overlaps with cached
        is_valid = True
        if new_first <= cached_last:
            # Get overlapping candles
            overlap_start = max(new_first, cached_last - pd.Timedelta(minutes=overlap_candles * 5))
            cached_overlap = cached_df[cached_df.index >= overlap_start].copy()
            new_overlap = new_df[new_df.index <= cached_last].copy()

            # Check if overlapping timestamps match
            common_timestamps = cached_overlap.index.intersection(new_overlap.index)
            if len(common_timestamps) > 0:
                # Compare OHLC values (allow small floating point differences)
                for ts in common_timestamps:
                    cached_row = cached_overlap.loc[ts]
                    new_row = new_overlap.loc[ts]

                    for col in ['open', 'high', 'low', 'close']:
                        if col in cached_row and col in new_row:
                            diff = abs(cached_row[col] - new_row[col])
                            rel_diff = diff / cached_row[col] if cached_row[col] != 0 else 0

                            # Allow 0.01% difference for floating point errors
                            if rel_diff > 0.0001:
                                logger.warning(
                                    f"Overlap mismatch at {ts}: {col} "
                                    f"cached={cached_row[col]:.4f} vs new={new_row[col]:.4f} "
                                    f"(diff={rel_diff:.4%})"
                                )
                                is_valid = False
                                break

                    if not is_valid:
                        break

        # Merge data (combine and remove duplicates, keeping newer values)
        merged = pd.concat([cached_df, new_df])
        merged = merged[~merged.index.duplicated(keep='last')]  # Keep newer values
        merged = merged.sort_index()

        return merged, is_valid

    def detect_gaps(
        self,
        df: pd.DataFrame,
        timeframe: str,
        expected_interval_minutes: Optional[int] = None
    ) -> List[Tuple[datetime, datetime]]:
        """
        Detect gaps in time series data.

        Args:
            df: DataFrame with datetime index
            timeframe: Timeframe string (e.g., "5m", "15m", "1h")
            expected_interval_minutes: Expected minutes between candles (auto-detected if None)

        Returns:
            List of (gap_start, gap_end) tuples representing missing data ranges
        """
        if df is None or len(df) < 2:
            return []

        # Parse expected interval from timeframe if not provided
        if expected_interval_minutes is None:
            expected_interval_minutes = self._parse_timeframe_minutes(timeframe)

        expected_delta = pd.Timedelta(minutes=expected_interval_minutes)

        # Calculate time differences between consecutive candles
        time_diffs = df.index.to_series().diff()

        # Find gaps (differences > 1.5x expected interval to account for market closures)
        # For intraday data, we expect some gaps on weekends
        gap_threshold = expected_delta * 1.5
        gap_mask = time_diffs > gap_threshold

        gaps = []
        for idx in df.index[gap_mask]:
            gap_end = idx
            gap_start = df.index[df.index < idx][-1] + expected_delta

            # Only report gaps during potential trading hours (Mon-Fri)
            # Skip weekend gaps (Fri close to Mon open)
            if not self._is_weekend_gap(gap_start, gap_end):
                gaps.append((gap_start, gap_end))

        return gaps

    def _parse_timeframe_minutes(self, timeframe: str) -> int:
        """Parse timeframe string to minutes."""
        timeframe = timeframe.lower()
        if timeframe.endswith('m'):
            return int(timeframe[:-1])
        elif timeframe.endswith('h'):
            return int(timeframe[:-1]) * 60
        elif timeframe.endswith('d'):
            return int(timeframe[:-1]) * 1440
        else:
            raise ValueError(f"Unknown timeframe format: {timeframe}")

    def _is_weekend_gap(self, start: datetime, end: datetime) -> bool:
        """Check if gap is a weekend gap (Friday close to Monday open)."""
        # If gap spans Friday evening to Monday morning, it's a weekend gap
        if start.weekday() == 4 and end.weekday() == 0:  # Friday to Monday
            # Check if it's roughly a 2-day gap (48-72 hours)
            gap_hours = (end - start).total_seconds() / 3600
            if 48 <= gap_hours <= 72:
                return True
        return False

    def get_required_fetch_range(
        self,
        symbol: str,
        timeframe: str,
        target_candles: int = 2400
    ) -> Tuple[Optional[datetime], int]:
        """
        Determine what data needs to be fetched.

        Args:
            symbol: Asset symbol
            timeframe: Timeframe
            target_candles: Desired total candles in cache

        Returns:
            (fetch_since, fetch_count) tuple
            - fetch_since: Fetch data from this timestamp (None = full fetch)
            - fetch_count: Number of candles to fetch
        """
        metadata = self.load_metadata(symbol)
        if 'timeframes' not in metadata or timeframe not in metadata['timeframes']:
            # No cached data, do full fetch
            return None, target_candles

        tf_meta = metadata['timeframes'][timeframe]
        cached_count = tf_meta.get('candle_count', 0)
        last_candle = pd.to_datetime(tf_meta.get('last_candle'))

        # Calculate how many candles we need to fetch
        if cached_count >= target_candles:
            # We have enough, just fetch new candles since last update
            # Add 10 candles for overlap validation
            fetch_since = last_candle - pd.Timedelta(minutes=10 * self._parse_timeframe_minutes(timeframe))
            return fetch_since, 20  # Fetch ~20 new candles for update
        else:
            # We need more historical data
            needed = target_candles - cached_count
            fetch_since = last_candle - pd.Timedelta(minutes=needed * self._parse_timeframe_minutes(timeframe))
            return fetch_since, needed + 10  # +10 for overlap

    def clear_cache(self, symbol: Optional[str] = None) -> None:
        """
        Clear cache.

        Args:
            symbol: Clear specific symbol, or all if None
        """
        if symbol:
            symbol_dir = self._get_symbol_dir(symbol)
            if symbol_dir.exists():
                import shutil
                shutil.rmtree(symbol_dir)
                logger.info(f"Cleared cache for {symbol}")
        else:
            if self.cache_dir.exists():
                import shutil
                shutil.rmtree(self.cache_dir)
                self.cache_dir.mkdir(parents=True, exist_ok=True)
                logger.info("Cleared entire cache")

    def get_cache_stats(self) -> Dict:
        """Get cache statistics."""
        stats = {
            'total_symbols': 0,
            'total_size_mb': 0,
            'symbols': {}
        }

        if not self.cache_dir.exists():
            return stats

        for symbol_dir in self.cache_dir.iterdir():
            if not symbol_dir.is_dir():
                continue

            symbol = symbol_dir.name.replace('_', '/')
            metadata = self.load_metadata(symbol)

            symbol_size = sum(f.stat().st_size for f in symbol_dir.rglob('*') if f.is_file())

            stats['symbols'][symbol] = {
                'size_mb': symbol_size / (1024 * 1024),
                'timeframes': metadata.get('timeframes', {})
            }
            stats['total_size_mb'] += symbol_size / (1024 * 1024)

        stats['total_symbols'] = len(stats['symbols'])
        return stats
