"""
Flask Backend for Elliott Wave Live Signals Web UI.

Provides REST API endpoints for:
- Scanning markets with cache support
- Claude CLI integration for signal analysis
- Real-time signal updates
"""

from flask import Flask, jsonify, request, send_from_directory
from flask_cors import CORS
from datetime import datetime, timezone, timedelta
import json
import logging
from typing import Dict, Any, List, Optional
import sys
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

# Add src to path
sys.path.insert(0, str(Path(__file__).parent))

from screener import AssetScreener
from time_series_extractor import extract_from_screening_dataframes
from claude_prompt_builder import ClaudePromptBuilder
from claude_integration import create_claude_trader, ClaudeIntegrationError
from trade_history_manager import TradeHistoryManager
from performance_metrics import calculate_performance
from data_quality import DataQualityAssessment, SystemQualityMonitor
from timeframe_engine import TimeframeEngine, TimeframeOptimizer
from data_cache import DataCache
from twelvedata_provider import TwelveDataProvider
import config

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)  # Enable CORS for React frontend

# Global instances
screener = AssetScreener()
prompt_builder = ClaudePromptBuilder()
trade_history = TradeHistoryManager()
data_cache_instance = DataCache()
data_quality_assessor = DataQualityAssessment()
quality_monitor = SystemQualityMonitor(data_cache_instance)
timeframe_engine = TimeframeEngine(data_cache_instance)
timeframe_optimizer = TimeframeOptimizer(timeframe_engine)
twelvedata_provider = TwelveDataProvider()

# Cache for market data
class MarketDataCache:
    """Simple in-memory cache for market data with TTL."""

    def __init__(self, ttl_minutes: int = 5):
        self.cache: Dict[str, Dict[str, Any]] = {}
        self.ttl = timedelta(minutes=ttl_minutes)

    def get(self, symbol: str) -> Optional[Dict[str, Any]]:
        """Get cached data if not expired."""
        if symbol not in self.cache:
            return None

        entry = self.cache[symbol]
        if datetime.now(timezone.utc) - entry['timestamp'] > self.ttl:
            # Expired
            del self.cache[symbol]
            return None

        return entry['data']

    def set(self, symbol: str, data: Dict[str, Any]) -> None:
        """Cache data with timestamp."""
        self.cache[symbol] = {
            'data': data,
            'timestamp': datetime.now(timezone.utc)
        }

    def clear(self) -> None:
        """Clear entire cache."""
        self.cache.clear()


cache = MarketDataCache(ttl_minutes=5)


def _fetch_symbol_data(symbol: str, use_cache: bool, delay: float = 0) -> tuple[str, Optional[Dict[str, Any]]]:
    """
    Fetch screening data for a single symbol (for parallel execution).

    Args:
        symbol: Symbol to fetch
        use_cache: Whether to use cache
        delay: Delay in seconds before fetching (for rate limiting)

    Returns: (symbol, screening_result or None)
    """
    try:
        # Rate limiting delay
        if delay > 0:
            time.sleep(delay)

        # Check cache first
        if use_cache:
            cached_data = cache.get(symbol)
            if cached_data:
                print(f"  ‚úÖ {symbol}: Using cached data", flush=True)
                return (symbol, cached_data['screening_result'])

        print(f"  üîÑ {symbol}: Fetching fresh data...", flush=True)
        screening_result = screener.screen_asset(symbol)

        if not screening_result:
            print(f"  ‚ùå {symbol}: No screening result", flush=True)
            return (symbol, None)

        print(f"  ‚úÖ {symbol}: Data fetched (Score: {screening_result['score']})", flush=True)

        # Cache the result
        if use_cache:
            cache.set(symbol, {
                'screening_result': screening_result,
                'dataframes': None
            })

        return (symbol, screening_result)

    except Exception as e:
        print(f"  ‚ùå {symbol}: Error - {e}", flush=True)
        logger.error(f"Error fetching {symbol}: {e}", exc_info=True)
        return (symbol, None)


@app.route('/')
def index():
    """Serve the frontend HTML."""
    frontend_dir = Path(__file__).parent / 'frontend'
    return send_from_directory(frontend_dir, 'index.html')


@app.route('/dashboard')
def dashboard():
    """Serve the world-class dashboard."""
    frontend_dir = Path(__file__).parent / 'frontend'
    return send_from_directory(frontend_dir, 'dashboard.html')


@app.route('/api/health', methods=['GET'])
def health_check():
    """Health check endpoint."""
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now(timezone.utc).isoformat()
    })


@app.route('/api/scan', methods=['POST'])
def scan_markets():
    """
    Scan markets and analyze signals.

    Request body:
    {
        "symbols": ["EUR/USD", "BTC/USD"],  // Optional, defaults to all
        "min_score": 75,                     // Optional
        "use_claude": true,                  // Optional, default false
        "use_cache": true                    // Optional, default true
    }
    """
    try:
        data = request.get_json() or {}
        symbols = data.get('symbols', config.ALL_ASSETS)
        min_score = data.get('min_score', 75)
        use_claude = data.get('use_claude', False)
        use_cache = data.get('use_cache', True)

        print(f"\n{'='*60}", flush=True)
        print(f"üîç STARTING SCAN", flush=True)
        print(f"{'='*60}", flush=True)
        print(f"Symbols: {symbols}", flush=True)
        print(f"Min Score: {min_score}", flush=True)
        print(f"Use Claude: {use_claude}", flush=True)
        print(f"Use Cache: {use_cache}", flush=True)
        print(f"{'='*60}\n", flush=True)

        logger.info(f"Scanning {len(symbols)} symbols (min_score: {min_score}, use_claude: {use_claude})")

        results = []
        scan_start = datetime.now(timezone.utc)

        # PHASE 1: Fetch data in small batches (avoids rate limits, still faster than sequential)
        print(f"\n{'='*60}", flush=True)
        print(f"üì• PHASE 1: FETCHING DATA (BATCHED PARALLEL, 2-3 AT A TIME)", flush=True)
        print(f"{'='*60}\n", flush=True)

        screening_results = {}
        fetch_start = datetime.now(timezone.utc)

        # Process symbols in batches of 2-3 to avoid overwhelming TradingView
        batch_size = 2
        delay_between_batches = 0.5  # 500ms delay between batches

        for batch_idx in range(0, len(symbols), batch_size):
            batch = symbols[batch_idx:batch_idx + batch_size]

            # Small delay before each batch (except first)
            if batch_idx > 0:
                time.sleep(delay_between_batches)

            print(f"\n  Batch {batch_idx//batch_size + 1}/{(len(symbols) + batch_size - 1)//batch_size}: {batch}", flush=True)

            # Fetch batch in parallel
            with ThreadPoolExecutor(max_workers=batch_size) as executor:
                futures = {executor.submit(_fetch_symbol_data, symbol, use_cache, 0): symbol for symbol in batch}

                for future in as_completed(futures):
                    symbol_result, screening_result = future.result()
                    if screening_result:
                        screening_results[symbol_result] = screening_result

        fetch_duration = (datetime.now(timezone.utc) - fetch_start).total_seconds()
        print(f"\n‚úÖ Data fetching complete: {len(screening_results)}/{len(symbols)} symbols in {fetch_duration:.1f}s", flush=True)

        # PHASE 2: Process results and analyze signals
        print(f"\n{'='*60}", flush=True)
        print(f"üîç PHASE 2: ANALYZING SIGNALS", flush=True)
        print(f"{'='*60}\n", flush=True)

        for symbol in symbols:
            try:
                screening_result = screening_results.get(symbol)
                if not screening_result:
                    continue

                score = screening_result['score']
                print(f"üìä {symbol}: Score = {score} (threshold: {min_score})", flush=True)

                # Only include if score >= threshold
                if score < min_score:
                    print(f"  ‚è≠Ô∏è  Score too low, skipping\n", flush=True)
                    continue

                print(f"  ‚ú® QUALIFIED! Processing...", flush=True)

                # Prepare result
                result = {
                    'symbol': symbol,
                    'category': _get_category(symbol),
                    'score': screening_result['score'],
                    'direction': screening_result['direction'],
                    'tradeable': screening_result['tradeable'],
                    'current_price': screening_result['current_price'],
                    'stop_loss': screening_result['stop_loss'],
                    'take_profit': screening_result['take_profit'],
                    'leverage': screening_result['leverage'],
                    'score_breakdown': screening_result['score_breakdown'],
                    'indicators': screening_result.get('indicators', {}),
                    'timestamp': screening_result['timestamp']
                }

                # If Claude analysis is requested for high-scoring signals
                if use_claude and screening_result['score'] >= 85:
                    print(f"  üß† High score ({screening_result['score']}), calling Claude CLI...", flush=True)
                    logger.info(f"{symbol}: High score ({screening_result['score']}), calling Claude CLI...")
                    claude_analysis = _analyze_with_claude(symbol, screening_result)

                    if claude_analysis:
                        result['claude_analysis'] = claude_analysis
                        result['enhanced_score'] = _calculate_enhanced_score(
                            screening_result['score'],
                            claude_analysis
                        )
                        print(f"  ‚úÖ Claude analysis complete (confidence: {claude_analysis.get('confidence', 0):.0%})\n", flush=True)
                    else:
                        print(f"  ‚ö†Ô∏è  Claude analysis failed\n", flush=True)
                else:
                    print(f"", flush=True)

                results.append(result)

            except Exception as e:
                logger.error(f"Error processing {symbol}: {e}", exc_info=True)
                print(f"  ‚ùå Error: {e}\n", flush=True)
                continue

        # Sort by score (or enhanced_score if available)
        results.sort(
            key=lambda x: x.get('enhanced_score', x['score']),
            reverse=True
        )

        scan_duration = (datetime.now(timezone.utc) - scan_start).total_seconds()

        print(f"\n{'='*60}", flush=True)
        print(f"‚úÖ SCAN COMPLETE", flush=True)
        print(f"{'='*60}", flush=True)
        print(f"Total Scanned: {len(symbols)}", flush=True)
        print(f"Qualified Signals: {len(results)}", flush=True)
        print(f"Duration: {scan_duration:.1f}s", flush=True)
        print(f"{'='*60}\n", flush=True)

        return jsonify({
            'success': True,
            'signals': results,
            'scan_info': {
                'total_scanned': len(symbols),
                'qualified_signals': len(results),
                'duration_seconds': scan_duration,
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'min_score': min_score,
                'used_claude': use_claude
            }
        })

    except Exception as e:
        logger.error(f"Error in scan_markets: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/analyze/<symbol>', methods=['POST'])
def analyze_signal(symbol: str):
    """
    Analyze a specific signal with Claude CLI.

    Request body:
    {
        "screening_result": {...}  // Optional, will screen if not provided
    }
    """
    try:
        data = request.get_json() or {}
        screening_result = data.get('screening_result')

        if not screening_result:
            # Run screener
            screening_result = screener.screen_asset(symbol)
            if not screening_result:
                return jsonify({
                    'success': False,
                    'error': f'No screening result for {symbol}'
                }), 404

        # Analyze with Claude
        claude_analysis = _analyze_with_claude(symbol, screening_result)

        if not claude_analysis:
            return jsonify({
                'success': False,
                'error': 'Claude analysis failed'
            }), 500

        # Calculate enhanced score
        enhanced_score = _calculate_enhanced_score(
            screening_result['score'],
            claude_analysis
        )

        return jsonify({
            'success': True,
            'symbol': symbol,
            'original_score': screening_result['score'],
            'enhanced_score': enhanced_score,
            'claude_analysis': claude_analysis
        })

    except Exception as e:
        logger.error(f"Error analyzing {symbol}: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/cache/clear', methods=['POST'])
def clear_cache():
    """Clear the market data cache."""
    cache.clear()
    return jsonify({
        'success': True,
        'message': 'Cache cleared'
    })


@app.route('/api/performance', methods=['GET'])
def get_performance():
    """Get trading performance metrics."""
    try:
        account_state = trade_history.get_account_state(initial_capital=100.0)
        performance = calculate_performance(
            trade_history.get_closed_trades(),
            account_state['available_capital'],
            initial_capital=100.0
        )

        return jsonify({
            'success': True,
            'performance': performance,
            'account_state': account_state
        })

    except Exception as e:
        logger.error(f"Error getting performance: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


def _get_category(symbol: str) -> str:
    """Determine asset category from symbol."""
    if '/' in symbol:
        if any(crypto in symbol for crypto in ['BTC', 'ETH', 'BNB', 'SOL']):
            return 'Crypto'
        else:
            return 'Forex Majors'
    elif 'XAU' in symbol or 'XAG' in symbol:
        return 'Commodities'
    else:
        return 'Other'


def _analyze_with_claude(symbol: str, screening_result: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    Analyze a signal with Claude CLI.

    Returns Claude's decision or None if analysis fails.
    """
    try:
        # Get fresh dataframes for time series extraction
        screening_dataframes = screener._fetch_multi_timeframe_data(symbol)
        if not screening_dataframes:
            logger.warning(f"No dataframes for {symbol}")
            return None

        screening_dataframes = screener._calculate_indicators(screening_dataframes)

        # Extract time series
        time_series_data = extract_from_screening_dataframes(screening_dataframes)

        # Get account state
        account_state = trade_history.get_account_state(initial_capital=100.0)

        # Get recent trades
        recent_trades = trade_history.get_recent_trades(count=5)

        # Calculate performance
        performance = calculate_performance(
            trade_history.get_closed_trades(),
            account_state['available_capital'],
            initial_capital=100.0
        )

        # Build prompts
        system_prompt, user_prompt = prompt_builder.get_complete_prompt(
            screening_result=screening_result,
            time_series_data=time_series_data,
            account_state=account_state,
            trade_history=recent_trades,
            performance_metrics=performance
        )

        # Call Claude CLI
        claude = create_claude_trader(method='cli', simulate=False)
        decision = claude.analyze_trade(
            system_prompt=system_prompt,
            user_prompt=user_prompt
        )

        # Include the prompts used for transparency
        decision['_prompts'] = {
            'system': system_prompt,
            'user': user_prompt
        }

        return decision

    except ClaudeIntegrationError as e:
        logger.error(f"Claude integration error for {symbol}: {e}")
        return None
    except Exception as e:
        logger.error(f"Error analyzing {symbol} with Claude: {e}", exc_info=True)
        return None


def _calculate_enhanced_score(base_score: int, claude_analysis: Dict[str, Any]) -> int:
    """
    Calculate enhanced score incorporating Claude's confidence and analysis.

    Logic:
    - High confidence (>0.8) + buy/sell signal: +10 points
    - Medium confidence (0.6-0.8) + buy/sell signal: +5 points
    - Low confidence (<0.6) or hold: +0 points
    - Concerns present: -5 points
    """
    enhanced = base_score

    signal = claude_analysis.get('signal', 'hold')
    confidence = claude_analysis.get('confidence', 0)
    concerns = claude_analysis.get('concerns', '')

    # Bonus for actionable signals with confidence
    if signal in ['buy', 'sell']:
        if confidence >= 0.8:
            enhanced += 10
        elif confidence >= 0.6:
            enhanced += 5

    # Penalty for concerns
    if concerns and len(concerns) > 20:  # Non-trivial concern
        enhanced -= 5

    # Cap at 194 (max possible score)
    return min(enhanced, 194)


# ============================================================================
# TWELVEDATA MONITORING ENDPOINTS
# ============================================================================

@app.route('/api/twelvedata/status', methods=['GET'])
def get_twelvedata_status():
    """
    Get TwelveData API status and metrics.

    Returns:
    {
        "api_status": "HEALTHY",
        "calls_today": 247,
        "calls_per_minute": 5,
        "rate_limit_health": 62,
        ...
    }
    """
    try:
        # Get call count and rate info
        calls_today = twelvedata_provider.get_call_count()
        calls_per_minute = twelvedata_provider.CALLS_PER_MINUTE
        daily_limit = 800  # Free tier limit

        # Calculate health metrics
        rate_limit_health = int(((daily_limit - calls_today) / daily_limit) * 100)

        # Determine status
        if calls_today < daily_limit * 0.7:
            api_status = "HEALTHY"
        elif calls_today < daily_limit * 0.9:
            api_status = "WARNING"
        else:
            api_status = "CRITICAL"

        # Get cache stats
        cache_stats = data_cache_instance.get_cache_stats()

        return jsonify({
            'success': True,
            'api_status': api_status,
            'calls_today': calls_today,
            'daily_limit': daily_limit,
            'calls_per_minute_limit': calls_per_minute,
            'rate_limit_health': rate_limit_health,
            'cache_stats': cache_stats,
            'timestamp': datetime.now(timezone.utc).isoformat()
        })

    except Exception as e:
        logger.error(f"Error getting TwelveData status: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/twelvedata/symbols', methods=['GET'])
def get_symbol_status():
    """
    Get per-symbol data status.

    Returns detailed status for each symbol including:
    - Last fetch time
    - Candle count
    - Data freshness
    - Cache status
    """
    try:
        symbols = config.ALL_ASSETS
        results = []

        for symbol in symbols:
            # Get metadata
            metadata = data_cache_instance.load_metadata(symbol)

            # Get 5m timeframe info (primary)
            tf_meta = metadata.get('timeframes', {}).get('5m', {})

            if tf_meta:
                last_updated = tf_meta.get('last_updated')
                candle_count = tf_meta.get('candle_count', 0)
                last_candle = tf_meta.get('last_candle')

                # Calculate freshness
                if last_updated:
                    last_update_dt = datetime.fromisoformat(last_updated.replace('Z', '+00:00'))
                    age_seconds = (datetime.now(timezone.utc) - last_update_dt).total_seconds()
                    age_minutes = int(age_seconds / 60)

                    if age_minutes < 5:
                        status = "LIVE"
                        status_emoji = "üü¢"
                    elif age_minutes < 15:
                        status = "STALE"
                        status_emoji = "üü°"
                    else:
                        status = "OLD"
                        status_emoji = "üî¥"

                    freshness = int((1 - min(age_minutes / 60, 1)) * 100)
                else:
                    age_minutes = 0
                    status = "UNKNOWN"
                    status_emoji = "‚ö´"
                    freshness = 0

                results.append({
                    'symbol': symbol,
                    'status': status,
                    'status_emoji': status_emoji,
                    'last_fetch_minutes': age_minutes,
                    'candles': candle_count,
                    'freshness': freshness,
                    'last_candle': last_candle
                })
            else:
                # No cached data
                results.append({
                    'symbol': symbol,
                    'status': "NO DATA",
                    'status_emoji': "‚ö´",
                    'last_fetch_minutes': None,
                    'candles': 0,
                    'freshness': 0,
                    'last_candle': None
                })

        return jsonify({
            'success': True,
            'symbols': results,
            'timestamp': datetime.now(timezone.utc).isoformat()
        })

    except Exception as e:
        logger.error(f"Error getting symbol status: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/twelvedata/refresh', methods=['POST'])
def refresh_symbol_data():
    """
    Force refresh data for specific symbols.

    Request body:
    {
        "symbols": ["EUR/USD", "GBP/USD"],  // Optional, defaults to all
        "timeframe": "5m"                    // Optional, defaults to 5m
    }
    """
    try:
        data = request.get_json() or {}
        symbols = data.get('symbols', config.ALL_ASSETS)
        timeframe = data.get('timeframe', '5m')

        logger.info(f"Refreshing data for {len(symbols)} symbols ({timeframe})")

        results = {}
        for symbol in symbols:
            try:
                # Clear cache for this symbol
                data_cache_instance.clear_cache(symbol)

                # Fetch fresh data via screener
                screening_result = screener.screen_asset(symbol)

                if screening_result:
                    results[symbol] = {
                        'success': True,
                        'score': screening_result['score'],
                        'timestamp': screening_result['timestamp']
                    }
                else:
                    results[symbol] = {
                        'success': False,
                        'error': 'Failed to fetch data'
                    }
            except Exception as e:
                logger.error(f"Error refreshing {symbol}: {e}")
                results[symbol] = {
                    'success': False,
                    'error': str(e)
                }

        return jsonify({
            'success': True,
            'results': results,
            'timestamp': datetime.now(timezone.utc).isoformat()
        })

    except Exception as e:
        logger.error(f"Error refreshing symbol data: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


# ============================================================================
# DATA QUALITY ENDPOINTS
# ============================================================================

@app.route('/api/quality/assess', methods=['POST'])
def assess_data_quality():
    """
    Assess data quality for specific symbol and timeframe.

    Request body:
    {
        "symbol": "EUR/USD",
        "timeframe": "5m",
        "target_candles": 2400  // Optional
    }
    """
    try:
        data = request.get_json() or {}
        symbol = data.get('symbol')
        timeframe = data.get('timeframe', '5m')
        target_candles = data.get('target_candles', 2400)

        if not symbol:
            return jsonify({
                'success': False,
                'error': 'Symbol is required'
            }), 400

        # Load data
        df = data_cache_instance.load_data(symbol, timeframe)
        if df is None or len(df) == 0:
            return jsonify({
                'success': False,
                'error': f'No data available for {symbol} {timeframe}'
            }), 404

        # Get metadata
        metadata = data_cache_instance.load_metadata(symbol)

        # Assess quality
        assessment = data_quality_assessor.assess_quality(
            symbol, timeframe, df, metadata, target_candles
        )

        return jsonify({
            'success': True,
            'assessment': assessment
        })

    except Exception as e:
        logger.error(f"Error assessing quality: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/quality/system', methods=['GET'])
def get_system_quality():
    """
    Get system-wide data quality assessment.

    Query params:
    - symbols: Comma-separated list (optional, defaults to all)
    - timeframes: Comma-separated list (optional, defaults to 5m)
    """
    try:
        symbols = request.args.get('symbols')
        if symbols:
            symbols = symbols.split(',')
        else:
            symbols = config.ALL_ASSETS

        timeframes = request.args.get('timeframes')
        if timeframes:
            timeframes = timeframes.split(',')
        else:
            timeframes = ['5m']

        # Assess all symbols
        assessment = quality_monitor.assess_all_symbols(symbols, timeframes)

        return jsonify({
            'success': True,
            'assessment': assessment
        })

    except Exception as e:
        logger.error(f"Error getting system quality: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/quality/startup', methods=['GET'])
def validate_startup_quality():
    """
    Quick startup quality validation.

    Checks primary timeframe (5m) for all symbols.
    """
    try:
        symbols = config.ALL_ASSETS

        # Run startup validation
        validation = quality_monitor.validate_on_startup(symbols, primary_timeframe='5m')

        return jsonify({
            'success': True,
            'validation': validation
        })

    except Exception as e:
        logger.error(f"Error validating startup quality: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


# ============================================================================
# TIMEFRAME ENGINE ENDPOINTS
# ============================================================================

@app.route('/api/timeframes/available', methods=['GET'])
def get_available_timeframes():
    """
    Get available timeframes for a symbol.

    Query params:
    - symbol: Asset symbol (required)
    - min_candles: Minimum candles required (optional, default 200)
    """
    try:
        symbol = request.args.get('symbol')
        min_candles = request.args.get('min_candles', type=int, default=200)

        if not symbol:
            return jsonify({
                'success': False,
                'error': 'Symbol is required'
            }), 400

        # Load 5m base data
        df_5m = data_cache_instance.load_data(symbol, '5m')
        if df_5m is None or len(df_5m) == 0:
            return jsonify({
                'success': False,
                'error': f'No 5m data available for {symbol}'
            }), 404

        # Calculate available timeframes
        available = timeframe_engine.get_available_timeframes(df_5m, min_candles)

        return jsonify({
            'success': True,
            'symbol': symbol,
            'source_candles': len(df_5m),
            'min_candles': min_candles,
            'available_timeframes': available
        })

    except Exception as e:
        logger.error(f"Error getting available timeframes: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/timeframes/calculate', methods=['POST'])
def calculate_custom_timeframe():
    """
    Calculate metadata for a custom timeframe.

    Request body:
    {
        "minutes": 45,           // Target timeframe in minutes
        "source_candles": 2400   // Available 5m candles
    }
    """
    try:
        data = request.get_json() or {}
        minutes = data.get('minutes')
        source_candles = data.get('source_candles')

        if minutes is None or source_candles is None:
            return jsonify({
                'success': False,
                'error': 'minutes and source_candles are required'
            }), 400

        # Calculate timeframe metadata
        result = timeframe_engine.calculate_custom_timeframe(minutes, source_candles)

        if 'error' in result:
            return jsonify({
                'success': False,
                'error': result['error']
            }), 400

        return jsonify({
            'success': True,
            'calculation': result
        })

    except Exception as e:
        logger.error(f"Error calculating custom timeframe: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/timeframes/resample', methods=['POST'])
def resample_timeframe():
    """
    Resample data to a target timeframe.

    Request body:
    {
        "symbol": "EUR/USD",
        "target_minutes": 45,
        "cache_result": true  // Optional, default true
    }
    """
    try:
        data = request.get_json() or {}
        symbol = data.get('symbol')
        target_minutes = data.get('target_minutes')
        cache_result = data.get('cache_result', True)

        if not symbol or target_minutes is None:
            return jsonify({
                'success': False,
                'error': 'symbol and target_minutes are required'
            }), 400

        # Load 5m data
        df_5m = data_cache_instance.load_data(symbol, '5m')
        if df_5m is None or len(df_5m) == 0:
            return jsonify({
                'success': False,
                'error': f'No 5m data available for {symbol}'
            }), 404

        # Resample
        target_tf = f"{target_minutes}m" if target_minutes < 60 else f"{target_minutes // 60}h"

        if cache_result:
            resampled = timeframe_engine.resample_and_cache(
                symbol, df_5m, target_tf, target_minutes
            )
        else:
            resampled = timeframe_engine.resample_to_timeframe(df_5m, target_minutes)

        if resampled is None:
            return jsonify({
                'success': False,
                'error': 'Resampling failed'
            }), 500

        # Validate quality
        ratio = target_minutes / 5
        validation = timeframe_engine.validate_resampled_quality(df_5m, resampled, ratio)

        return jsonify({
            'success': True,
            'symbol': symbol,
            'target_timeframe': target_tf,
            'source_candles': len(df_5m),
            'resampled_candles': len(resampled),
            'cached': cache_result,
            'validation': validation
        })

    except Exception as e:
        logger.error(f"Error resampling timeframe: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/timeframes/recommend', methods=['GET'])
def recommend_timeframes():
    """
    Recommend optimal timeframes for a strategy.

    Query params:
    - symbol: Asset symbol (required)
    - strategy: Strategy type (aggressive/balanced/conservative, default balanced)
    """
    try:
        symbol = request.args.get('symbol')
        strategy = request.args.get('strategy', 'balanced')

        if not symbol:
            return jsonify({
                'success': False,
                'error': 'Symbol is required'
            }), 400

        if strategy not in ['aggressive', 'balanced', 'conservative']:
            return jsonify({
                'success': False,
                'error': 'Invalid strategy (must be aggressive, balanced, or conservative)'
            }), 400

        # Load 5m data
        df_5m = data_cache_instance.load_data(symbol, '5m')
        if df_5m is None or len(df_5m) == 0:
            return jsonify({
                'success': False,
                'error': f'No 5m data available for {symbol}'
            }), 404

        # Get recommendations
        recommendations = timeframe_optimizer.recommend_timeframes(df_5m, strategy)

        # Calculate requirements
        requirements = timeframe_optimizer.calculate_data_requirements(recommendations)

        return jsonify({
            'success': True,
            'symbol': symbol,
            'strategy': strategy,
            'recommendations': recommendations,
            'requirements': requirements
        })

    except Exception as e:
        logger.error(f"Error recommending timeframes: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(description='Elliott Wave Web Backend')
    parser.add_argument('--host', default='127.0.0.1', help='Host to bind to')
    parser.add_argument('--port', type=int, default=5000, help='Port to bind to')
    parser.add_argument('--debug', action='store_true', help='Enable debug mode')

    args = parser.parse_args()

    logger.info(f"Starting Flask backend on {args.host}:{args.port}")
    app.run(host=args.host, port=args.port, debug=args.debug)
