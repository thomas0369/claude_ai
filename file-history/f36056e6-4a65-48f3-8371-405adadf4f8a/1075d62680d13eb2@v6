"""
Flask Backend for Elliott Wave Live Signals Web UI.

Provides REST API endpoints for:
- Scanning markets with cache support
- Claude CLI integration for signal analysis
- Real-time signal updates
"""

from flask import Flask, jsonify, request, send_from_directory
from flask_cors import CORS
from datetime import datetime, timezone, timedelta
import json
import logging
from typing import Dict, Any, List, Optional
import sys
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import time

# Add src to path
sys.path.insert(0, str(Path(__file__).parent))

from screener import AssetScreener
from time_series_extractor import extract_from_screening_dataframes
from claude_prompt_builder import ClaudePromptBuilder
from claude_integration import create_claude_trader, ClaudeIntegrationError
from trade_history_manager import TradeHistoryManager
from performance_metrics import calculate_performance
from data_quality import DataQualityAssessment, SystemQualityMonitor
from timeframe_engine import TimeframeEngine, TimeframeOptimizer
from data_cache import DataCache
from twelvedata_provider import TwelveDataProvider
from claude_validation_queue import ClaudeValidationQueue
from signal_history import SignalHistoryManager
from export_manager import ExportManager
import config

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)  # Enable CORS for React frontend

# Global instances
screener = AssetScreener()
prompt_builder = ClaudePromptBuilder()
trade_history = TradeHistoryManager()
data_cache_instance = DataCache()
data_quality_assessor = DataQualityAssessment()
quality_monitor = SystemQualityMonitor(data_cache_instance)
timeframe_engine = TimeframeEngine(data_cache_instance)
timeframe_optimizer = TimeframeOptimizer(timeframe_engine)
twelvedata_provider = TwelveDataProvider()
claude_validation_queue = ClaudeValidationQueue(
    max_validations_per_day=20,
    min_score_for_validation=85,
    cache_dir="data/claude_cache"
)
signal_history_manager = SignalHistoryManager(db_path="data/signal_history.db")
export_manager = ExportManager(
    output_dir="exports",
    smtp_host=config.SMTP_HOST if hasattr(config, 'SMTP_HOST') else None,
    smtp_port=config.SMTP_PORT if hasattr(config, 'SMTP_PORT') else 587,
    smtp_user=config.SMTP_USER if hasattr(config, 'SMTP_USER') else None,
    smtp_password=config.SMTP_PASSWORD if hasattr(config, 'SMTP_PASSWORD') else None
)

# Cache for market data
class MarketDataCache:
    """Simple in-memory cache for market data with TTL."""

    def __init__(self, ttl_minutes: int = 5):
        self.cache: Dict[str, Dict[str, Any]] = {}
        self.ttl = timedelta(minutes=ttl_minutes)

    def get(self, symbol: str) -> Optional[Dict[str, Any]]:
        """Get cached data if not expired."""
        if symbol not in self.cache:
            return None

        entry = self.cache[symbol]
        if datetime.now(timezone.utc) - entry['timestamp'] > self.ttl:
            # Expired
            del self.cache[symbol]
            return None

        return entry['data']

    def set(self, symbol: str, data: Dict[str, Any]) -> None:
        """Cache data with timestamp."""
        self.cache[symbol] = {
            'data': data,
            'timestamp': datetime.now(timezone.utc)
        }

    def clear(self) -> None:
        """Clear entire cache."""
        self.cache.clear()


cache = MarketDataCache(ttl_minutes=5)


def _fetch_symbol_data(symbol: str, use_cache: bool, delay: float = 0) -> tuple[str, Optional[Dict[str, Any]]]:
    """
    Fetch screening data for a single symbol (for parallel execution).

    Args:
        symbol: Symbol to fetch
        use_cache: Whether to use cache
        delay: Delay in seconds before fetching (for rate limiting)

    Returns: (symbol, screening_result or None)
    """
    try:
        # Rate limiting delay
        if delay > 0:
            time.sleep(delay)

        # Check cache first
        if use_cache:
            cached_data = cache.get(symbol)
            if cached_data:
                print(f"  ‚úÖ {symbol}: Using cached data", flush=True)
                return (symbol, cached_data['screening_result'])

        print(f"  üîÑ {symbol}: Fetching fresh data...", flush=True)
        screening_result = screener.screen_asset(symbol)

        if not screening_result:
            print(f"  ‚ùå {symbol}: No screening result", flush=True)
            return (symbol, None)

        print(f"  ‚úÖ {symbol}: Data fetched (Score: {screening_result['score']})", flush=True)

        # Cache the result
        if use_cache:
            cache.set(symbol, {
                'screening_result': screening_result,
                'dataframes': None
            })

        return (symbol, screening_result)

    except Exception as e:
        print(f"  ‚ùå {symbol}: Error - {e}", flush=True)
        logger.error(f"Error fetching {symbol}: {e}", exc_info=True)
        return (symbol, None)


@app.route('/')
def index():
    """Serve the frontend HTML."""
    frontend_dir = Path(__file__).parent / 'frontend'
    return send_from_directory(frontend_dir, 'index.html')


@app.route('/dashboard')
def dashboard():
    """Serve the world-class dashboard."""
    frontend_dir = Path(__file__).parent / 'frontend'
    return send_from_directory(frontend_dir, 'dashboard.html')


@app.route('/api/health', methods=['GET'])
def health_check():
    """Health check endpoint."""
    return jsonify({
        'status': 'healthy',
        'timestamp': datetime.now(timezone.utc).isoformat()
    })


@app.route('/api/scan', methods=['POST'])
def scan_markets():
    """
    Scan markets and analyze signals.

    Request body:
    {
        "symbols": ["EUR/USD", "BTC/USD"],  // Optional, defaults to all
        "min_score": 75,                     // Optional
        "use_claude": true,                  // Optional, default false
        "use_cache": true                    // Optional, default true
    }
    """
    try:
        data = request.get_json() or {}
        symbols = data.get('symbols', config.ALL_ASSETS)
        min_score = data.get('min_score', 75)
        use_claude = data.get('use_claude', False)
        use_cache = data.get('use_cache', True)

        print(f"\n{'='*60}", flush=True)
        print(f"üîç STARTING SCAN", flush=True)
        print(f"{'='*60}", flush=True)
        print(f"Symbols: {symbols}", flush=True)
        print(f"Min Score: {min_score}", flush=True)
        print(f"Use Claude: {use_claude}", flush=True)
        print(f"Use Cache: {use_cache}", flush=True)
        print(f"{'='*60}\n", flush=True)

        logger.info(f"Scanning {len(symbols)} symbols (min_score: {min_score}, use_claude: {use_claude})")

        results = []
        scan_start = datetime.now(timezone.utc)

        # PHASE 1: Fetch data in small batches (avoids rate limits, still faster than sequential)
        print(f"\n{'='*60}", flush=True)
        print(f"üì• PHASE 1: FETCHING DATA (BATCHED PARALLEL, 2-3 AT A TIME)", flush=True)
        print(f"{'='*60}\n", flush=True)

        screening_results = {}
        fetch_start = datetime.now(timezone.utc)

        # Process symbols in batches of 2-3 to avoid overwhelming TradingView
        batch_size = 2
        delay_between_batches = 0.5  # 500ms delay between batches

        for batch_idx in range(0, len(symbols), batch_size):
            batch = symbols[batch_idx:batch_idx + batch_size]

            # Small delay before each batch (except first)
            if batch_idx > 0:
                time.sleep(delay_between_batches)

            print(f"\n  Batch {batch_idx//batch_size + 1}/{(len(symbols) + batch_size - 1)//batch_size}: {batch}", flush=True)

            # Fetch batch in parallel
            with ThreadPoolExecutor(max_workers=batch_size) as executor:
                futures = {executor.submit(_fetch_symbol_data, symbol, use_cache, 0): symbol for symbol in batch}

                for future in as_completed(futures):
                    symbol_result, screening_result = future.result()
                    if screening_result:
                        screening_results[symbol_result] = screening_result

        fetch_duration = (datetime.now(timezone.utc) - fetch_start).total_seconds()
        print(f"\n‚úÖ Data fetching complete: {len(screening_results)}/{len(symbols)} symbols in {fetch_duration:.1f}s", flush=True)

        # PHASE 2: Process results and analyze signals
        print(f"\n{'='*60}", flush=True)
        print(f"üîç PHASE 2: ANALYZING SIGNALS", flush=True)
        print(f"{'='*60}\n", flush=True)

        for symbol in symbols:
            try:
                screening_result = screening_results.get(symbol)
                if not screening_result:
                    continue

                score = screening_result['score']
                print(f"üìä {symbol}: Score = {score} (threshold: {min_score})", flush=True)

                # Only include if score >= threshold
                if score < min_score:
                    print(f"  ‚è≠Ô∏è  Score too low, skipping\n", flush=True)
                    continue

                print(f"  ‚ú® QUALIFIED! Processing...", flush=True)

                # Prepare result
                result = {
                    'symbol': symbol,
                    'category': _get_category(symbol),
                    'score': screening_result['score'],
                    'direction': screening_result['direction'],
                    'tradeable': screening_result['tradeable'],
                    'current_price': screening_result['current_price'],
                    'stop_loss': screening_result['stop_loss'],
                    'take_profit': screening_result['take_profit'],
                    'leverage': screening_result['leverage'],
                    'score_breakdown': screening_result['score_breakdown'],
                    'indicators': screening_result.get('indicators', {}),
                    'timestamp': screening_result['timestamp']
                }

                # If Claude analysis is requested for high-scoring signals
                if use_claude and screening_result['score'] >= claude_validation_queue.min_score:
                    print(f"  üß† High score ({screening_result['score']}), checking Claude validation...", flush=True)
                    logger.info(f"{symbol}: High score ({screening_result['score']}), checking Claude validation...")

                    # Check if should validate and if cached
                    should_validate, reason = claude_validation_queue.should_validate(
                        symbol=symbol,
                        score=screening_result['score']
                    )

                    if should_validate:
                        # Check cache first
                        cached_result = claude_validation_queue.get_cached_result(symbol, screening_result['score'])

                        if cached_result:
                            print(f"  ‚úÖ Using cached Claude validation (confidence: {cached_result.confidence:.0%})", flush=True)
                            result['claude_analysis'] = {
                                'signal': cached_result.decision,
                                'confidence': cached_result.confidence,
                                'justification': cached_result.justification,
                                'concerns': cached_result.concerns,
                                'cached': True
                            }
                            result['enhanced_score'] = cached_result.enhanced_score
                        else:
                            # Enqueue or validate immediately
                            print(f"  üîÑ Validating with Claude CLI...", flush=True)
                            claude_analysis = _analyze_with_claude(symbol, screening_result)

                            if claude_analysis:
                                # Calculate enhanced score
                                enhanced_score = _calculate_enhanced_score(
                                    screening_result['score'],
                                    claude_analysis
                                )

                                # Cache the result
                                decision_time = claude_analysis.get('_decision_time', 3.0)
                                cached = claude_validation_queue.cache_validation_result(
                                    symbol=symbol,
                                    base_score=screening_result['score'],
                                    validation_result={
                                        'decision': claude_analysis.get('signal', 'hold'),
                                        'confidence': claude_analysis.get('confidence', 0),
                                        'justification': claude_analysis.get('justification', ''),
                                        'concerns': claude_analysis.get('concerns', ''),
                                        'enhanced_score': enhanced_score
                                    },
                                    decision_time=decision_time
                                )

                                result['claude_analysis'] = claude_analysis
                                result['enhanced_score'] = enhanced_score
                                print(f"  ‚úÖ Claude validation complete (confidence: {claude_analysis.get('confidence', 0):.0%})\n", flush=True)
                            else:
                                print(f"  ‚ö†Ô∏è  Claude validation failed\n", flush=True)
                    else:
                        print(f"  ‚è≠Ô∏è  {reason}\n", flush=True)
                else:
                    print(f"", flush=True)

                results.append(result)

            except Exception as e:
                logger.error(f"Error processing {symbol}: {e}", exc_info=True)
                print(f"  ‚ùå Error: {e}\n", flush=True)
                continue

        # Sort by score (or enhanced_score if available)
        results.sort(
            key=lambda x: x.get('enhanced_score', x['score']),
            reverse=True
        )

        scan_duration = (datetime.now(timezone.utc) - scan_start).total_seconds()

        print(f"\n{'='*60}", flush=True)
        print(f"‚úÖ SCAN COMPLETE", flush=True)
        print(f"{'='*60}", flush=True)
        print(f"Total Scanned: {len(symbols)}", flush=True)
        print(f"Qualified Signals: {len(results)}", flush=True)
        print(f"Duration: {scan_duration:.1f}s", flush=True)
        print(f"{'='*60}\n", flush=True)

        return jsonify({
            'success': True,
            'signals': results,
            'scan_info': {
                'total_scanned': len(symbols),
                'qualified_signals': len(results),
                'duration_seconds': scan_duration,
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'min_score': min_score,
                'used_claude': use_claude
            }
        })

    except Exception as e:
        logger.error(f"Error in scan_markets: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/analyze/<symbol>', methods=['POST'])
def analyze_signal(symbol: str):
    """
    Analyze a specific signal with Claude CLI.

    Request body:
    {
        "screening_result": {...}  // Optional, will screen if not provided
    }
    """
    try:
        data = request.get_json() or {}
        screening_result = data.get('screening_result')

        if not screening_result:
            # Run screener
            screening_result = screener.screen_asset(symbol)
            if not screening_result:
                return jsonify({
                    'success': False,
                    'error': f'No screening result for {symbol}'
                }), 404

        # Analyze with Claude
        claude_analysis = _analyze_with_claude(symbol, screening_result)

        if not claude_analysis:
            return jsonify({
                'success': False,
                'error': 'Claude analysis failed'
            }), 500

        # Calculate enhanced score
        enhanced_score = _calculate_enhanced_score(
            screening_result['score'],
            claude_analysis
        )

        return jsonify({
            'success': True,
            'symbol': symbol,
            'original_score': screening_result['score'],
            'enhanced_score': enhanced_score,
            'claude_analysis': claude_analysis
        })

    except Exception as e:
        logger.error(f"Error analyzing {symbol}: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/cache/clear', methods=['POST'])
def clear_cache():
    """Clear the market data cache."""
    cache.clear()
    return jsonify({
        'success': True,
        'message': 'Cache cleared'
    })


@app.route('/api/performance', methods=['GET'])
def get_performance():
    """Get trading performance metrics."""
    try:
        account_state = trade_history.get_account_state(initial_capital=100.0)
        performance = calculate_performance(
            trade_history.get_closed_trades(),
            account_state['available_capital'],
            initial_capital=100.0
        )

        return jsonify({
            'success': True,
            'performance': performance,
            'account_state': account_state
        })

    except Exception as e:
        logger.error(f"Error getting performance: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


def _get_category(symbol: str) -> str:
    """Determine asset category from symbol."""
    if '/' in symbol:
        if any(crypto in symbol for crypto in ['BTC', 'ETH', 'BNB', 'SOL']):
            return 'Crypto'
        else:
            return 'Forex Majors'
    elif 'XAU' in symbol or 'XAG' in symbol:
        return 'Commodities'
    else:
        return 'Other'


def _analyze_with_claude(symbol: str, screening_result: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    Analyze a signal with Claude CLI.

    Returns Claude's decision or None if analysis fails.
    """
    try:
        # Get fresh dataframes for time series extraction
        screening_dataframes = screener._fetch_multi_timeframe_data(symbol)
        if not screening_dataframes:
            logger.warning(f"No dataframes for {symbol}")
            return None

        screening_dataframes = screener._calculate_indicators(screening_dataframes)

        # Extract time series
        time_series_data = extract_from_screening_dataframes(screening_dataframes)

        # Get account state
        account_state = trade_history.get_account_state(initial_capital=100.0)

        # Get recent trades
        recent_trades = trade_history.get_recent_trades(count=5)

        # Calculate performance
        performance = calculate_performance(
            trade_history.get_closed_trades(),
            account_state['available_capital'],
            initial_capital=100.0
        )

        # Build prompts
        system_prompt, user_prompt = prompt_builder.get_complete_prompt(
            screening_result=screening_result,
            time_series_data=time_series_data,
            account_state=account_state,
            trade_history=recent_trades,
            performance_metrics=performance
        )

        # Call Claude CLI
        claude = create_claude_trader(method='cli', simulate=False)
        decision = claude.analyze_trade(
            system_prompt=system_prompt,
            user_prompt=user_prompt
        )

        # Include the prompts used for transparency
        decision['_prompts'] = {
            'system': system_prompt,
            'user': user_prompt
        }

        return decision

    except ClaudeIntegrationError as e:
        logger.error(f"Claude integration error for {symbol}: {e}")
        return None
    except Exception as e:
        logger.error(f"Error analyzing {symbol} with Claude: {e}", exc_info=True)
        return None


def _calculate_enhanced_score(base_score: int, claude_analysis: Dict[str, Any]) -> int:
    """
    Calculate enhanced score incorporating Claude's confidence and analysis.

    Logic:
    - High confidence (>0.8) + buy/sell signal: +10 points
    - Medium confidence (0.6-0.8) + buy/sell signal: +5 points
    - Low confidence (<0.6) or hold: +0 points
    - Concerns present: -5 points
    """
    enhanced = base_score

    signal = claude_analysis.get('signal', 'hold')
    confidence = claude_analysis.get('confidence', 0)
    concerns = claude_analysis.get('concerns', '')

    # Bonus for actionable signals with confidence
    if signal in ['buy', 'sell']:
        if confidence >= 0.8:
            enhanced += 10
        elif confidence >= 0.6:
            enhanced += 5

    # Penalty for concerns
    if concerns and len(concerns) > 20:  # Non-trivial concern
        enhanced -= 5

    # Cap at 194 (max possible score)
    return min(enhanced, 194)


# ============================================================================
# TWELVEDATA MONITORING ENDPOINTS
# ============================================================================

@app.route('/api/twelvedata/status', methods=['GET'])
def get_twelvedata_status():
    """
    Get TwelveData API status and metrics.

    Returns:
    {
        "api_status": "HEALTHY",
        "calls_today": 247,
        "calls_per_minute": 5,
        "rate_limit_health": 62,
        ...
    }
    """
    try:
        # Get call count and rate info
        calls_today = twelvedata_provider.get_call_count()
        calls_per_minute = twelvedata_provider.CALLS_PER_MINUTE
        daily_limit = 800  # Free tier limit

        # Calculate health metrics
        rate_limit_health = int(((daily_limit - calls_today) / daily_limit) * 100)

        # Determine status
        if calls_today < daily_limit * 0.7:
            api_status = "HEALTHY"
        elif calls_today < daily_limit * 0.9:
            api_status = "WARNING"
        else:
            api_status = "CRITICAL"

        # Get cache stats
        cache_stats = data_cache_instance.get_cache_stats()

        return jsonify({
            'success': True,
            'api_status': api_status,
            'calls_today': calls_today,
            'daily_limit': daily_limit,
            'calls_per_minute_limit': calls_per_minute,
            'rate_limit_health': rate_limit_health,
            'cache_stats': cache_stats,
            'timestamp': datetime.now(timezone.utc).isoformat()
        })

    except Exception as e:
        logger.error(f"Error getting TwelveData status: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/twelvedata/symbols', methods=['GET'])
def get_symbol_status():
    """
    Get per-symbol data status.

    Returns detailed status for each symbol including:
    - Last fetch time
    - Candle count
    - Data freshness
    - Cache status
    """
    try:
        symbols = config.ALL_ASSETS
        results = []

        for symbol in symbols:
            # Get metadata
            metadata = data_cache_instance.load_metadata(symbol)

            # Get 5m timeframe info (primary)
            tf_meta = metadata.get('timeframes', {}).get('5m', {})

            if tf_meta:
                last_updated = tf_meta.get('last_updated')
                candle_count = tf_meta.get('candle_count', 0)
                last_candle = tf_meta.get('last_candle')

                # Calculate freshness
                if last_updated:
                    last_update_dt = datetime.fromisoformat(last_updated.replace('Z', '+00:00'))
                    age_seconds = (datetime.now(timezone.utc) - last_update_dt).total_seconds()
                    age_minutes = int(age_seconds / 60)

                    if age_minutes < 5:
                        status = "LIVE"
                        status_emoji = "üü¢"
                    elif age_minutes < 15:
                        status = "STALE"
                        status_emoji = "üü°"
                    else:
                        status = "OLD"
                        status_emoji = "üî¥"

                    freshness = int((1 - min(age_minutes / 60, 1)) * 100)
                else:
                    age_minutes = 0
                    status = "UNKNOWN"
                    status_emoji = "‚ö´"
                    freshness = 0

                results.append({
                    'symbol': symbol,
                    'status': status,
                    'status_emoji': status_emoji,
                    'last_fetch_minutes': age_minutes,
                    'candles': candle_count,
                    'freshness': freshness,
                    'last_candle': last_candle
                })
            else:
                # No cached data
                results.append({
                    'symbol': symbol,
                    'status': "NO DATA",
                    'status_emoji': "‚ö´",
                    'last_fetch_minutes': None,
                    'candles': 0,
                    'freshness': 0,
                    'last_candle': None
                })

        return jsonify({
            'success': True,
            'symbols': results,
            'timestamp': datetime.now(timezone.utc).isoformat()
        })

    except Exception as e:
        logger.error(f"Error getting symbol status: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/twelvedata/refresh', methods=['POST'])
def refresh_symbol_data():
    """
    Force refresh data for specific symbols.

    Request body:
    {
        "symbols": ["EUR/USD", "GBP/USD"],  // Optional, defaults to all
        "timeframe": "5m"                    // Optional, defaults to 5m
    }
    """
    try:
        data = request.get_json() or {}
        symbols = data.get('symbols', config.ALL_ASSETS)
        timeframe = data.get('timeframe', '5m')

        logger.info(f"Refreshing data for {len(symbols)} symbols ({timeframe})")

        results = {}
        for symbol in symbols:
            try:
                # Clear cache for this symbol
                data_cache_instance.clear_cache(symbol)

                # Fetch fresh data via screener
                screening_result = screener.screen_asset(symbol)

                if screening_result:
                    results[symbol] = {
                        'success': True,
                        'score': screening_result['score'],
                        'timestamp': screening_result['timestamp']
                    }
                else:
                    results[symbol] = {
                        'success': False,
                        'error': 'Failed to fetch data'
                    }
            except Exception as e:
                logger.error(f"Error refreshing {symbol}: {e}")
                results[symbol] = {
                    'success': False,
                    'error': str(e)
                }

        return jsonify({
            'success': True,
            'results': results,
            'timestamp': datetime.now(timezone.utc).isoformat()
        })

    except Exception as e:
        logger.error(f"Error refreshing symbol data: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


# ============================================================================
# DATA QUALITY ENDPOINTS
# ============================================================================

@app.route('/api/quality/assess', methods=['POST'])
def assess_data_quality():
    """
    Assess data quality for specific symbol and timeframe.

    Request body:
    {
        "symbol": "EUR/USD",
        "timeframe": "5m",
        "target_candles": 2400  // Optional
    }
    """
    try:
        data = request.get_json() or {}
        symbol = data.get('symbol')
        timeframe = data.get('timeframe', '5m')
        target_candles = data.get('target_candles', 2400)

        if not symbol:
            return jsonify({
                'success': False,
                'error': 'Symbol is required'
            }), 400

        # Load data
        df = data_cache_instance.load_data(symbol, timeframe)
        if df is None or len(df) == 0:
            return jsonify({
                'success': False,
                'error': f'No data available for {symbol} {timeframe}'
            }), 404

        # Get metadata
        metadata = data_cache_instance.load_metadata(symbol)

        # Assess quality
        assessment = data_quality_assessor.assess_quality(
            symbol, timeframe, df, metadata, target_candles
        )

        return jsonify({
            'success': True,
            'assessment': assessment
        })

    except Exception as e:
        logger.error(f"Error assessing quality: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/quality/system', methods=['GET'])
def get_system_quality():
    """
    Get system-wide data quality assessment.

    Query params:
    - symbols: Comma-separated list (optional, defaults to all)
    - timeframes: Comma-separated list (optional, defaults to 5m)
    """
    try:
        symbols = request.args.get('symbols')
        if symbols:
            symbols = symbols.split(',')
        else:
            symbols = config.ALL_ASSETS

        timeframes = request.args.get('timeframes')
        if timeframes:
            timeframes = timeframes.split(',')
        else:
            timeframes = ['5m']

        # Assess all symbols
        assessment = quality_monitor.assess_all_symbols(symbols, timeframes)

        return jsonify({
            'success': True,
            'assessment': assessment
        })

    except Exception as e:
        logger.error(f"Error getting system quality: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/quality/startup', methods=['GET'])
def validate_startup_quality():
    """
    Quick startup quality validation.

    Checks primary timeframe (5m) for all symbols.
    """
    try:
        symbols = config.ALL_ASSETS

        # Run startup validation
        validation = quality_monitor.validate_on_startup(symbols, primary_timeframe='5m')

        return jsonify({
            'success': True,
            'validation': validation
        })

    except Exception as e:
        logger.error(f"Error validating startup quality: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


# ============================================================================
# TIMEFRAME ENGINE ENDPOINTS
# ============================================================================

@app.route('/api/timeframes/available', methods=['GET'])
def get_available_timeframes():
    """
    Get available timeframes for a symbol.

    Query params:
    - symbol: Asset symbol (required)
    - min_candles: Minimum candles required (optional, default 200)
    """
    try:
        symbol = request.args.get('symbol')
        min_candles = request.args.get('min_candles', type=int, default=200)

        if not symbol:
            return jsonify({
                'success': False,
                'error': 'Symbol is required'
            }), 400

        # Load 5m base data
        df_5m = data_cache_instance.load_data(symbol, '5m')
        if df_5m is None or len(df_5m) == 0:
            return jsonify({
                'success': False,
                'error': f'No 5m data available for {symbol}'
            }), 404

        # Calculate available timeframes
        available = timeframe_engine.get_available_timeframes(df_5m, min_candles)

        return jsonify({
            'success': True,
            'symbol': symbol,
            'source_candles': len(df_5m),
            'min_candles': min_candles,
            'available_timeframes': available
        })

    except Exception as e:
        logger.error(f"Error getting available timeframes: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/timeframes/calculate', methods=['POST'])
def calculate_custom_timeframe():
    """
    Calculate metadata for a custom timeframe.

    Request body:
    {
        "minutes": 45,           // Target timeframe in minutes
        "source_candles": 2400   // Available 5m candles
    }
    """
    try:
        data = request.get_json() or {}
        minutes = data.get('minutes')
        source_candles = data.get('source_candles')

        if minutes is None or source_candles is None:
            return jsonify({
                'success': False,
                'error': 'minutes and source_candles are required'
            }), 400

        # Calculate timeframe metadata
        result = timeframe_engine.calculate_custom_timeframe(minutes, source_candles)

        if 'error' in result:
            return jsonify({
                'success': False,
                'error': result['error']
            }), 400

        return jsonify({
            'success': True,
            'calculation': result
        })

    except Exception as e:
        logger.error(f"Error calculating custom timeframe: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/timeframes/resample', methods=['POST'])
def resample_timeframe():
    """
    Resample data to a target timeframe.

    Request body:
    {
        "symbol": "EUR/USD",
        "target_minutes": 45,
        "cache_result": true  // Optional, default true
    }
    """
    try:
        data = request.get_json() or {}
        symbol = data.get('symbol')
        target_minutes = data.get('target_minutes')
        cache_result = data.get('cache_result', True)

        if not symbol or target_minutes is None:
            return jsonify({
                'success': False,
                'error': 'symbol and target_minutes are required'
            }), 400

        # Load 5m data
        df_5m = data_cache_instance.load_data(symbol, '5m')
        if df_5m is None or len(df_5m) == 0:
            return jsonify({
                'success': False,
                'error': f'No 5m data available for {symbol}'
            }), 404

        # Resample
        target_tf = f"{target_minutes}m" if target_minutes < 60 else f"{target_minutes // 60}h"

        if cache_result:
            resampled = timeframe_engine.resample_and_cache(
                symbol, df_5m, target_tf, target_minutes
            )
        else:
            resampled = timeframe_engine.resample_to_timeframe(df_5m, target_minutes)

        if resampled is None:
            return jsonify({
                'success': False,
                'error': 'Resampling failed'
            }), 500

        # Validate quality
        ratio = target_minutes / 5
        validation = timeframe_engine.validate_resampled_quality(df_5m, resampled, ratio)

        return jsonify({
            'success': True,
            'symbol': symbol,
            'target_timeframe': target_tf,
            'source_candles': len(df_5m),
            'resampled_candles': len(resampled),
            'cached': cache_result,
            'validation': validation
        })

    except Exception as e:
        logger.error(f"Error resampling timeframe: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/timeframes/recommend', methods=['GET'])
def recommend_timeframes():
    """
    Recommend optimal timeframes for a strategy.

    Query params:
    - symbol: Asset symbol (required)
    - strategy: Strategy type (aggressive/balanced/conservative, default balanced)
    """
    try:
        symbol = request.args.get('symbol')
        strategy = request.args.get('strategy', 'balanced')

        if not symbol:
            return jsonify({
                'success': False,
                'error': 'Symbol is required'
            }), 400

        if strategy not in ['aggressive', 'balanced', 'conservative']:
            return jsonify({
                'success': False,
                'error': 'Invalid strategy (must be aggressive, balanced, or conservative)'
            }), 400

        # Load 5m data
        df_5m = data_cache_instance.load_data(symbol, '5m')
        if df_5m is None or len(df_5m) == 0:
            return jsonify({
                'success': False,
                'error': f'No 5m data available for {symbol}'
            }), 404

        # Get recommendations
        recommendations = timeframe_optimizer.recommend_timeframes(df_5m, strategy)

        # Calculate requirements
        requirements = timeframe_optimizer.calculate_data_requirements(recommendations)

        return jsonify({
            'success': True,
            'symbol': symbol,
            'strategy': strategy,
            'recommendations': recommendations,
            'requirements': requirements
        })

    except Exception as e:
        logger.error(f"Error recommending timeframes: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


# ============================================================================
# CLAUDE VALIDATION QUEUE ENDPOINTS
# ============================================================================

@app.route('/api/claude/enqueue', methods=['POST'])
def enqueue_claude_validation():
    """
    Add a validation request to the Claude queue.

    Request body:
    {
        "symbol": "EUR/USD",
        "screening_result": {...},  // Full screening result from screener
        "force": false              // Optional, bypass checks
    }

    Returns:
    {
        "success": true,
        "enqueued": true,
        "reason": "High score signal queued for validation",
        "position": 3,
        "queue_status": {...}
    }
    """
    try:
        data = request.get_json() or {}
        symbol = data.get('symbol')
        screening_result = data.get('screening_result')
        force = data.get('force', False)

        if not symbol or not screening_result:
            return jsonify({
                'success': False,
                'error': 'symbol and screening_result are required'
            }), 400

        # Check if should validate (if not forcing)
        if not force:
            score = screening_result.get('score', 0)
            should_validate, reason = claude_validation_queue.should_validate(
                symbol=symbol,
                score=score
            )

            if not should_validate:
                return jsonify({
                    'success': True,
                    'enqueued': False,
                    'reason': reason,
                    'queue_status': claude_validation_queue.get_queue_status()
                })

        # Enqueue the validation request
        enqueued, reason = claude_validation_queue.enqueue_validation(
            symbol=symbol,
            screening_result=screening_result
        )

        # Get queue position
        queue_status = claude_validation_queue.get_queue_status()
        position = len(queue_status['queue']) if enqueued else None

        return jsonify({
            'success': True,
            'enqueued': enqueued,
            'reason': reason,
            'position': position,
            'queue_status': queue_status
        })

    except Exception as e:
        logger.error(f"Error enqueuing Claude validation: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/claude/queue/status', methods=['GET'])
def get_claude_queue_status():
    """
    Get current status of the Claude validation queue.

    Returns:
    {
        "success": true,
        "status": {
            "queue_size": 3,
            "validations_today": 8,
            "max_validations": 20,
            "quota_remaining": 12,
            "cache_size": 15,
            "queue": [...]  // Array of pending validation requests
        }
    }
    """
    try:
        queue_status = claude_validation_queue.get_queue_status()

        # Transform to match API contract
        status = {
            'queue_size': queue_status['queue_length'],
            'validations_today': queue_status['validations_today'],
            'max_validations': queue_status['daily_limit'],
            'quota_remaining': queue_status['validations_remaining'],
            'cache_size': queue_status['cache_size'],
            'queue': queue_status['pending_requests']
        }

        return jsonify({
            'success': True,
            'status': status
        })

    except Exception as e:
        logger.error(f"Error getting Claude queue status: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/claude/performance', methods=['GET'])
def get_claude_performance():
    """
    Get Claude validation performance statistics.

    Returns:
    {
        "success": true,
        "performance": {
            "total_validations": 145,
            "avg_confidence": 0.78,
            "avg_enhancement": 7.2,
            "avg_decision_time": 3.4,
            "validation_outcomes": {
                "approve": 98,
                "reject": 47
            },
            "high_confidence_rate": 0.62
        }
    }
    """
    try:
        performance = claude_validation_queue.get_performance_stats()

        return jsonify({
            'success': True,
            'performance': performance
        })

    except Exception as e:
        logger.error(f"Error getting Claude performance: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/claude/settings', methods=['GET', 'POST'])
def manage_claude_settings():
    """
    Get or update Claude validation queue settings.

    GET: Returns current settings
    POST: Updates settings

    Request body (POST):
    {
        "max_validations_per_day": 20,    // Optional
        "min_score_for_validation": 85,   // Optional
        "cache_ttl_minutes": 60           // Optional
    }
    """
    try:
        if request.method == 'GET':
            # Return current settings
            return jsonify({
                'success': True,
                'settings': {
                    'max_validations_per_day': claude_validation_queue.max_validations,
                    'min_score_for_validation': claude_validation_queue.min_score,
                    'cache_ttl_minutes': claude_validation_queue.cache_ttl.total_seconds() / 60
                }
            })

        elif request.method == 'POST':
            data = request.get_json() or {}

            # Update settings
            if 'max_validations_per_day' in data:
                max_val = data['max_validations_per_day']
                if not isinstance(max_val, int) or max_val < 1 or max_val > 100:
                    return jsonify({
                        'success': False,
                        'error': 'max_validations_per_day must be between 1 and 100'
                    }), 400
                claude_validation_queue.max_validations = max_val

            if 'min_score_for_validation' in data:
                min_score = data['min_score_for_validation']
                if not isinstance(min_score, int) or min_score < 50 or min_score > 194:
                    return jsonify({
                        'success': False,
                        'error': 'min_score_for_validation must be between 50 and 194'
                    }), 400
                claude_validation_queue.min_score = min_score

            if 'cache_ttl_minutes' in data:
                ttl_min = data['cache_ttl_minutes']
                if not isinstance(ttl_min, (int, float)) or ttl_min < 1 or ttl_min > 1440:
                    return jsonify({
                        'success': False,
                        'error': 'cache_ttl_minutes must be between 1 and 1440 (24 hours)'
                    }), 400
                claude_validation_queue.cache_ttl = timedelta(minutes=ttl_min)

            # Persist updated settings
            claude_validation_queue._save_state()

            return jsonify({
                'success': True,
                'message': 'Settings updated successfully',
                'settings': {
                    'max_validations_per_day': claude_validation_queue.max_validations,
                    'min_score_for_validation': claude_validation_queue.min_score,
                    'cache_ttl_minutes': claude_validation_queue.cache_ttl.total_seconds() / 60
                }
            })

    except Exception as e:
        logger.error(f"Error managing Claude settings: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


# ============================================================================
# SIGNAL HISTORY TRACKING ENDPOINTS
# ============================================================================

@app.route('/api/history/signal', methods=['POST'])
def add_signal_to_history():
    """
    Add a new trading signal to history database.

    Request body:
    {
        "symbol": "EUR/USD",
        "direction": "long",
        "score": 145,
        "current_price": 1.0850,
        "entry_price": 1.0845,
        "stop_loss": 1.0825,
        "take_profit": 1.0895,
        "leverage": 10,
        "score_breakdown": {...},
        "indicators": {...},
        "timeframe": "1h",
        "enhanced_score": 155,  // Optional (if Claude validated)
        "claude_confidence": 0.85,  // Optional (if Claude validated)
        "claude_justification": "Strong wave structure...",  // Optional (if Claude validated)
        "data_quality_score": 95  // Optional
    }

    Returns:
    {
        "success": true,
        "signal_id": "EUR/USD_20250104_153045",
        "message": "Signal added to history"
    }
    """
    try:
        data = request.get_json() or {}

        # Required fields
        required = ['symbol', 'direction', 'score', 'current_price',
                   'entry_price', 'stop_loss', 'take_profit', 'leverage',
                   'score_breakdown', 'indicators', 'timeframe']

        missing = [f for f in required if f not in data]
        if missing:
            return jsonify({
                'success': False,
                'error': f'Missing required fields: {", ".join(missing)}'
            }), 400

        # Add signal to history
        signal_id = signal_history_manager.add_signal(
            symbol=data['symbol'],
            direction=data['direction'],
            score=data['score'],
            current_price=data['current_price'],
            entry_price=data['entry_price'],
            stop_loss=data['stop_loss'],
            take_profit=data['take_profit'],
            leverage=data['leverage'],
            score_breakdown=data['score_breakdown'],
            indicators=data['indicators'],
            timeframe=data['timeframe'],
            enhanced_score=data.get('enhanced_score'),
            claude_confidence=data.get('claude_confidence'),
            claude_justification=data.get('claude_justification'),
            data_quality_score=data.get('data_quality_score')
        )

        logger.info(f"Added signal to history: {signal_id}")

        return jsonify({
            'success': True,
            'signal_id': signal_id,
            'message': 'Signal added to history'
        })

    except Exception as e:
        logger.error(f"Error adding signal to history: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/history/signals', methods=['GET'])
def get_signal_history():
    """
    Get recent signal history.

    Query params:
    - limit: Number of signals to return (default 50, max 500)
    - symbol: Filter by symbol (optional)
    - status: Filter by status (pending/active/closed_win/closed_loss/stopped_out, optional)
    - days: Filter signals from last N days (default 30)
    - claude_only: Return only Claude-validated signals (optional, default false)

    Returns:
    {
        "success": true,
        "signals": [
            {
                "signal_id": "EUR/USD_20250104_153045",
                "timestamp": "2025-01-04T15:30:45Z",
                "symbol": "EUR/USD",
                "direction": "long",
                "score": 145,
                "enhanced_score": 155,
                "status": "closed_win",
                "outcome": "win",
                "pnl_percentage": 18.43,
                ...
            },
            ...
        ],
        "count": 15
    }
    """
    try:
        limit = request.args.get('limit', type=int, default=50)
        limit = min(limit, 500)  # Cap at 500

        symbol = request.args.get('symbol')
        status = request.args.get('status')
        days = request.args.get('days', type=int, default=30)
        claude_only = request.args.get('claude_only', type=bool, default=False)

        # Get signals from database
        signals = signal_history_manager.get_signals(
            limit=limit,
            symbol=symbol,
            status=status,
            days=days,
            claude_validated=claude_only if claude_only else None
        )

        return jsonify({
            'success': True,
            'signals': signals,
            'count': len(signals)
        })

    except Exception as e:
        logger.error(f"Error getting signal history: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/history/performance', methods=['GET'])
def get_history_performance():
    """
    Get performance statistics from signal history.

    Query params:
    - days: Calculate stats for last N days (default 30)
    - symbol: Filter by symbol (optional)

    Returns:
    {
        "success": true,
        "performance": {
            "total_signals": 47,
            "closed_signals": 32,
            "wins": 21,
            "losses": 11,
            "breakeven": 0,
            "win_rate": 0.656,
            "avg_pnl_percentage": 12.4,
            "total_pnl_points": 842.5,
            "ab_testing": {
                "screener_only": {
                    "total": 18,
                    "wins": 10,
                    "win_rate": 0.556,
                    "avg_pnl": 8.2
                },
                "claude_validated": {
                    "total": 14,
                    "wins": 11,
                    "win_rate": 0.786,
                    "avg_pnl": 17.8
                },
                "improvement": {
                    "win_rate_increase": 0.230,
                    "pnl_increase": 9.6
                }
            },
            "by_symbol": {...},
            "by_timeframe": {...}
        }
    }
    """
    try:
        days = request.args.get('days', type=int, default=30)
        symbol = request.args.get('symbol')

        # Get performance stats
        performance = signal_history_manager.get_performance_stats(
            days=days,
            symbol=symbol
        )

        return jsonify({
            'success': True,
            'performance': performance
        })

    except Exception as e:
        logger.error(f"Error getting history performance: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/history/timeline', methods=['GET'])
def get_history_timeline():
    """
    Get historical timeline with time-grouped statistics.

    Query params:
    - days: Get timeline for last N days (default 30)
    - group_by: Time grouping (day/week/month, default day)

    Returns:
    {
        "success": true,
        "timeline": [
            {
                "period": "2025-01-04",
                "signals": 3,
                "wins": 2,
                "losses": 1,
                "win_rate": 0.667,
                "avg_pnl": 14.2
            },
            ...
        ]
    }
    """
    try:
        days = request.args.get('days', type=int, default=30)
        group_by = request.args.get('group_by', default='day')

        if group_by not in ['day', 'week', 'month']:
            return jsonify({
                'success': False,
                'error': 'Invalid group_by (must be day, week, or month)'
            }), 400

        # Get timeline
        timeline = signal_history_manager.get_historical_timeline(
            days=days,
            group_by=group_by
        )

        return jsonify({
            'success': True,
            'timeline': timeline
        })

    except Exception as e:
        logger.error(f"Error getting history timeline: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/history/signal/<signal_id>/outcome', methods=['PUT'])
def update_signal_outcome(signal_id: str):
    """
    Update signal outcome when trade is closed.

    Request body:
    {
        "outcome": "win",  // win/loss/breakeven
        "actual_exit_price": 1.0895,
        "actual_exit_time": "2025-01-04T18:45:30Z",  // Optional, defaults to now
        "notes": "Hit TP exactly"  // Optional
    }

    Returns:
    {
        "success": true,
        "signal_id": "EUR/USD_20250104_153045",
        "outcome": "win",
        "pnl_points": 50.0,
        "pnl_percentage": 18.43,
        "message": "Signal outcome updated"
    }
    """
    try:
        data = request.get_json() or {}

        # Required fields
        if 'outcome' not in data or 'actual_exit_price' not in data:
            return jsonify({
                'success': False,
                'error': 'outcome and actual_exit_price are required'
            }), 400

        outcome = data['outcome']
        if outcome not in ['win', 'loss', 'breakeven']:
            return jsonify({
                'success': False,
                'error': 'outcome must be win, loss, or breakeven'
            }), 400

        # Update signal
        updated = signal_history_manager.update_signal_outcome(
            signal_id=signal_id,
            outcome=outcome,
            actual_exit_price=data['actual_exit_price'],
            actual_exit_time=data.get('actual_exit_time'),
            notes=data.get('notes')
        )

        if not updated:
            return jsonify({
                'success': False,
                'error': f'Signal not found: {signal_id}'
            }), 404

        # Get updated signal to return PnL
        signals = signal_history_manager.get_signals(limit=1, signal_id=signal_id)
        if signals:
            signal = signals[0]
            logger.info(f"Updated signal outcome: {signal_id} -> {outcome} (PnL: {signal.get('pnl_percentage', 0):.2f}%)")

            return jsonify({
                'success': True,
                'signal_id': signal_id,
                'outcome': outcome,
                'pnl_points': signal.get('pnl_points'),
                'pnl_percentage': signal.get('pnl_percentage'),
                'message': 'Signal outcome updated'
            })
        else:
            return jsonify({
                'success': True,
                'signal_id': signal_id,
                'message': 'Signal outcome updated'
            })

    except Exception as e:
        logger.error(f"Error updating signal outcome: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


# ============================================================================
# EXPORT & REPORTING ENDPOINTS
# ============================================================================

@app.route('/api/export/signal/pdf', methods=['POST'])
def export_signal_pdf():
    """
    Generate PDF report for a signal.

    Request body:
    {
        "signal_data": {...},  // Signal information
        "filename": "report.pdf"  // Optional custom filename
    }
    """
    try:
        data = request.get_json() or {}
        signal_data = data.get('signal_data')

        if not signal_data:
            return jsonify({
                'success': False,
                'error': 'signal_data is required'
            }), 400

        filename = data.get('filename')
        pdf_path = export_manager.generate_signal_report_pdf(signal_data, filename)

        return jsonify({
            'success': True,
            'filepath': pdf_path,
            'filename': os.path.basename(pdf_path),
            'message': 'PDF report generated'
        })

    except Exception as e:
        logger.error(f"Error generating PDF report: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/export/performance/pdf', methods=['POST'])
def export_performance_pdf():
    """
    Generate PDF performance report.

    Request body:
    {
        "days": 30,  // Optional, defaults to 30
        "symbol": "EUR/USD"  // Optional, for all symbols if omitted
    }
    """
    try:
        data = request.get_json() or {}
        days = data.get('days', 30)
        symbol = data.get('symbol')

        # Get performance data
        performance = signal_history_manager.get_performance_stats(days=days, symbol=symbol)

        # Get recent signals
        signals = signal_history_manager.get_signals(limit=20, days=days, symbol=symbol)

        # Generate PDF
        pdf_path = export_manager.generate_performance_report_pdf(performance, signals)

        return jsonify({
            'success': True,
            'filepath': pdf_path,
            'filename': os.path.basename(pdf_path),
            'message': 'Performance report generated'
        })

    except Exception as e:
        logger.error(f"Error generating performance PDF: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/export/signals/csv', methods=['POST'])
def export_signals_csv():
    """
    Export signals to CSV.

    Request body:
    {
        "limit": 100,  // Optional, defaults to 50
        "days": 30,  // Optional, defaults to 30
        "symbol": "EUR/USD"  // Optional
    }
    """
    try:
        data = request.get_json() or {}
        limit = data.get('limit', 50)
        days = data.get('days', 30)
        symbol = data.get('symbol')

        # Get signals
        signals = signal_history_manager.get_signals(limit=limit, days=days, symbol=symbol)

        # Export to CSV
        csv_path = export_manager.export_signals_to_csv(signals)

        return jsonify({
            'success': True,
            'filepath': csv_path,
            'filename': os.path.basename(csv_path),
            'count': len(signals),
            'message': f'Exported {len(signals)} signals to CSV'
        })

    except Exception as e:
        logger.error(f"Error exporting signals to CSV: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/export/performance/csv', methods=['POST'])
def export_performance_csv():
    """Export performance data to CSV."""
    try:
        data = request.get_json() or {}
        days = data.get('days', 30)

        # Get performance data
        performance = signal_history_manager.get_performance_stats(days=days)

        # Export to CSV
        csv_path = export_manager.export_performance_to_csv(performance)

        return jsonify({
            'success': True,
            'filepath': csv_path,
            'filename': os.path.basename(csv_path),
            'message': 'Performance data exported to CSV'
        })

    except Exception as e:
        logger.error(f"Error exporting performance to CSV: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/export/notify', methods=['POST'])
def send_email_notification():
    """
    Send email notification for a signal.

    Request body:
    {
        "to_email": "trader@example.com",
        "signal_data": {...},
        "attach_pdf": true  // Optional, defaults to true
    }
    """
    try:
        data = request.get_json() or {}
        to_email = data.get('to_email')
        signal_data = data.get('signal_data')
        attach_pdf = data.get('attach_pdf', True)

        if not to_email or not signal_data:
            return jsonify({
                'success': False,
                'error': 'to_email and signal_data are required'
            }), 400

        # Send notification
        sent = export_manager.send_signal_notification(
            to_email=to_email,
            signal_data=signal_data,
            attach_pdf=attach_pdf
        )

        if sent:
            return jsonify({
                'success': True,
                'message': f'Notification sent to {to_email}'
            })
        else:
            return jsonify({
                'success': False,
                'error': 'Email not configured or failed to send'
            }), 500

    except Exception as e:
        logger.error(f"Error sending notification: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/export/report/email', methods=['POST'])
def email_performance_report():
    """
    Email performance report.

    Request body:
    {
        "to_email": "trader@example.com",
        "days": 30  // Optional
    }
    """
    try:
        data = request.get_json() or {}
        to_email = data.get('to_email')
        days = data.get('days', 30)

        if not to_email:
            return jsonify({
                'success': False,
                'error': 'to_email is required'
            }), 400

        # Get data
        performance = signal_history_manager.get_performance_stats(days=days)
        signals = signal_history_manager.get_signals(limit=20, days=days)

        # Send report
        sent = export_manager.send_performance_report(
            to_email=to_email,
            performance_data=performance,
            signals=signals,
            period_days=days
        )

        if sent:
            return jsonify({
                'success': True,
                'message': f'Performance report sent to {to_email}'
            })
        else:
            return jsonify({
                'success': False,
                'error': 'Email not configured or failed to send'
            }), 500

    except Exception as e:
        logger.error(f"Error emailing performance report: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/export/list', methods=['GET'])
def list_exports():
    """
    List all export files.

    Query params:
    - type: Filter by type (pdf/csv, optional)
    """
    try:
        file_type = request.args.get('type')

        exports = export_manager.list_exports(file_type=file_type)

        return jsonify({
            'success': True,
            'exports': exports,
            'count': len(exports)
        })

    except Exception as e:
        logger.error(f"Error listing exports: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/export/download/<filename>', methods=['GET'])
def download_export(filename: str):
    """Download an export file."""
    try:
        from flask import send_from_directory

        return send_from_directory(
            export_manager.output_dir,
            filename,
            as_attachment=True
        )

    except FileNotFoundError:
        return jsonify({
            'success': False,
            'error': f'File not found: {filename}'
        }), 404
    except Exception as e:
        logger.error(f"Error downloading export: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/export/delete/<filename>', methods=['DELETE'])
def delete_export(filename: str):
    """Delete an export file."""
    try:
        deleted = export_manager.delete_export(filename)

        if deleted:
            return jsonify({
                'success': True,
                'message': f'Deleted {filename}'
            })
        else:
            return jsonify({
                'success': False,
                'error': f'File not found: {filename}'
            }), 404

    except Exception as e:
        logger.error(f"Error deleting export: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/export/cleanup', methods=['POST'])
def cleanup_old_exports():
    """
    Clean up old export files.

    Request body:
    {
        "days": 30  // Delete files older than this (default 30)
    }
    """
    try:
        data = request.get_json() or {}
        days = data.get('days', 30)

        deleted_count = export_manager.cleanup_old_exports(days=days)

        return jsonify({
            'success': True,
            'deleted_count': deleted_count,
            'message': f'Cleaned up {deleted_count} files older than {days} days'
        })

    except Exception as e:
        logger.error(f"Error cleaning up exports: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(description='Elliott Wave Web Backend')
    parser.add_argument('--host', default='127.0.0.1', help='Host to bind to')
    parser.add_argument('--port', type=int, default=5000, help='Port to bind to')
    parser.add_argument('--debug', action='store_true', help='Enable debug mode')

    args = parser.parse_args()

    logger.info(f"Starting Flask backend on {args.host}:{args.port}")
    app.run(host=args.host, port=args.port, debug=args.debug)
